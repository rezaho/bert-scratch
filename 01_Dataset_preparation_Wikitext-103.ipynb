{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import tarfile\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Extract the Data¶\n",
    "\n",
    "The dataset will be downloaded from the [Data Asset Exchange](https://developer.ibm.com/exchanges/data/) content delivery network and extract the tarball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4754e3d0483e4b2c8bcc1abd7b00de98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23144.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "fname = 'wikitext-103.tar.gz'\n",
    "output_dir = './wikidata/'\n",
    "url = 'https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/' + fname\n",
    "chunk_len = 8192\n",
    "with requests.get(url, stream=True) as r:\n",
    "    with open(output_dir + fname, 'wb') as f:\n",
    "        for chunk in tqdm(r.iter_content(chunk_size=chunk_len), total=int(r.headers['Content-Length'])//chunk_len):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "# Path(fname).write_bytes(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the file was downloaded properly by comparing sha512 checksums\n",
    "sha512sum = 'c8186919aa1840af6b734ea41abc580574ea8efe2fafda220f5d01002464d17566d84be5199b875136c9593f0e0678fb5d7c84bb2231de8b4151cb9c83fa2109'\n",
    "sha512sum_computed = hashlib.sha512(Path(output_dir + fname).read_bytes()).hexdigest()\n",
    "sha512sum == sha512sum_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset\n",
    "with tarfile.open(output_dir + fname) as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data\n",
    "Lets read our data into Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow!!\n"
     ]
    }
   ],
   "source": [
    "if re.match(r\"[\\s\\n\\t]+\", \"r \"):\n",
    "    print(\"Wow!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "def is_empty(elm):\n",
    "    return True if (re.match(r\"[\\s\\n\\t]+\", elm) or elm==\"\" or elm is None) else False\n",
    "\n",
    "class Wikitext103Handler():\n",
    "    \n",
    "    def __init__(self, wiki103_dir):\n",
    "        assert os.path.isdir(wiki103_dir) == True , \\\n",
    "        f\"The specified directory was not found!\"\n",
    "        assert os.path.isfile(os.path.join(wiki103_dir, \"wiki.train.tokens\")), \\\n",
    "        f\"Wikitext103 Train Tokens file Was Not Found in the path {wiki103_dir}!\"\n",
    "        assert os.path.isfile(os.path.join(wiki103_dir, \"wiki.valid.tokens\")), \\\n",
    "        f\"Wikitext103 Valid Tokens file Was Not Found in the path {wiki103_dir}!\"\n",
    "        assert os.path.isfile(os.path.join(wiki103_dir, \"wiki.test.tokens\")), \\\n",
    "        f\"Wikitext103 Test Tokens file Was Not Found in the path {wiki103_dir}!\"\n",
    "\n",
    "        # Reading data from disk\n",
    "        train_data = \" \\n\" + Path(os.path.join(wiki103_dir, \"wiki.train.tokens\")).read_text()\n",
    "        print(\"Train data was loaded successfuly! ...\")\n",
    "        valid_data = \" \\n\" + Path(os.path.join(wiki103_dir, \"wiki.valid.tokens\")).read_text()\n",
    "        print(\"Validation data was loaded successfuly! ...\")\n",
    "        test_data = \" \\n\" + Path(os.path.join(wiki103_dir, \"wiki.test.tokens\")).read_text()\n",
    "        print(\"Test data was loaded successfuly! ...\")\n",
    "\n",
    "\n",
    "        # Preprocessing data\n",
    "        ## Split out train headings and articles\n",
    "        self.train_data = self._preprocess_data(train_data)\n",
    "        self.train_data.update(dict(type='train'))\n",
    "        ## Split out valid headings and articles\n",
    "        self.valid_data = self._preprocess_data(valid_data)\n",
    "        self.valid_data.update(dict(type=\"valid\"))\n",
    "        ## Split out train headings and articles\n",
    "        self.test_data = self._preprocess_data(test_data)\n",
    "        self.test_data.update(dict(type=\"test\"))\n",
    "\n",
    "        # Set mode to \"training\" by default\n",
    "        self.data = self.train_data\n",
    "        \n",
    "        # Returns headings by default\n",
    "        self.return_headings = True\n",
    "    \n",
    "    def _preprocess_data(self, data):\n",
    "        # Store regular expression pattern to search for wikipedia article headings\n",
    "        heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'\n",
    "\n",
    "        split = re.split(heading_pattern, data)\n",
    "        headings = [x[7:-7] for x in split[1::2]]\n",
    "        articles = [\n",
    "            str(re.sub(r\"(\\n [= ]+ .*[ =]+ \\n)\", \"\", x)) # Removing subheadings\n",
    "            for x in split[2::2]\n",
    "        ]\n",
    "        articles = [\n",
    "            self.split_doc_to_sent(doc) for doc in tqdm(articles)\n",
    "        ]\n",
    "        return dict(\n",
    "            headings = headings,\n",
    "            articles = articles\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.return_headings:\n",
    "            return self.data['articles'][idx], self.data['headings'][idx]\n",
    "        return self.data['articles'][idx]\n",
    "    \n",
    "    \n",
    "    def parallel_process(self, iterator, func, ncores=None, message=None):\n",
    "        # from pathos.multiprocessing import ProcessPool as Pool\n",
    "        p = Pool(ncores)\n",
    "        iter_multi = tqdm(\n",
    "            p.imap(func, iterator),\n",
    "            total=len(iterator), \n",
    "            desc=message\n",
    "        )\n",
    "        \n",
    "        return list(iter_multi)\n",
    "    \n",
    "    \n",
    "    def split_doc_to_sent(self, doc, min_len=10):\n",
    "        return [\n",
    "            y.strip() for x in re.split(\"\\n  \\n\", doc) for y in re.split('\\n', x.strip()) \n",
    "            if not is_empty(y.strip()) and len(y.strip().split())>=min_len\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def set_mode(self, mode=None, return_headings=None):\n",
    "        if mode is not None:\n",
    "            if mode == 'train':\n",
    "                self.data = self.train_data\n",
    "            elif mode == 'eval':\n",
    "                self.data = self.valid_data\n",
    "            elif mode == 'test':\n",
    "                self.data = self.test_data\n",
    "            else:\n",
    "                raise TypeError('Parameter `mode` can only get one of the following values: \"train\", \"eval\", \"test\".')\n",
    "        \n",
    "        if return_headings is not None:\n",
    "            self.return_headings = return_headings     \n",
    "    \n",
    "    def get_mode(self):\n",
    "        mode = \"eval\" if self.data['type']==\"valid\" else self.data['type']\n",
    "        return mode\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_handler(handler_path):\n",
    "        with open(handler_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def save_handler(self, handler_path):\n",
    "        with open(handler_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "            \n",
    "    def export_for_bert(self, dir_path, unk_token=\"[UNK]\"):\n",
    "        def export(path, data, msg):\n",
    "            with open(path, 'w') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                for sent_col in self._process_for_bert(data, msg=msg):\n",
    "                    writer.writerow(sent_col)\n",
    "        # Export Train Data\n",
    "        export(\n",
    "            os.path.join(dir_path, \"wikitext103_bert_train.csv\"), \n",
    "            self.train_data['articles'], \n",
    "            msg=\"Export Train Data...\"\n",
    "        )\n",
    "        print(\"Train data export completed successfully!\\n\")\n",
    "        # Export Validation Data\n",
    "        export(\n",
    "            os.path.join(dir_path, \"wikitext103_bert_valid.csv\"), \n",
    "            self.valid_data['articles'], \n",
    "            msg=\"Export Valid Data...\"\n",
    "        )\n",
    "        print(\"Validation data export completed successfully!\\n\")\n",
    "        # Export Test Data\n",
    "        export(\n",
    "            os.path.join(dir_path, \"wikitext103_bert_test.csv\"), \n",
    "            self.test_data['articles'], \n",
    "            msg=\"Export Test Data...\"\n",
    "        )\n",
    "        print(\"Test data export completed successfully!\\n\")\n",
    "    \n",
    "    def _process_for_bert(self, articles, unk_token=\"[UNK]\", msg=None):\n",
    "        for article in tqdm(articles, total=len(articles), desc=msg):\n",
    "            for i, sent in enumerate(article):\n",
    "                if not is_empty(sent):\n",
    "                    yield (sent.replace(\"<unk>\", unk_token), 0 if i==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data was loaded successfuly! ...\n",
      "Validation data was loaded successfuly! ...\n",
      "Test data was loaded successfuly! ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c9adc20dfb4289903f2034e0592310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28471.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94069ea705144a71b3f1a0784d734bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c677951250544ccb7a51510887391f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wikitext = Wikitext103Handler(wiki103_dir=\"./wikitext-103\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2c82c044a249cd980468ab9c9cfb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export Train Data...', max=28471.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train data export completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e3cfee79fc4a7fafebeaed0b5adfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export Valid Data...', max=60.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data export completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd467c8e38f249a99bfc5ae726c0c259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export Test Data...', max=60.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test data export completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikitext.export_for_bert(dir_path=\"./wikitext-103\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_path = \"./wikitext-103/wiki_handler.pkl\"\n",
    "# wikitext.set_mode(mode=\"eval\", return_headings=False)\n",
    "wikitext.save_handler(handler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikitext = Wikitext103Handler.load_handler(handler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare For BERT WordPiece Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794,232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senjō no Valkyria 3 : [UNK] Chronicles ( Japan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The game began development in 2010 , carrying ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  Senjō no Valkyria 3 : [UNK] Chronicles ( Japan...  0\n",
       "1  The game began development in 2010 , carrying ...  1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./wikitext-103/wikitext103_bert_train.csv\"\n",
    "\n",
    "train_df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "print(\"{:,}\".format(train_df.shape[0]))\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68cb155a03446bfa4420b1607bacbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794232.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./wikitext-103/wikitext103_bert_train_for_tokenizer.txt\", 'w') as f:\n",
    "    for idx, row in tqdm(train_df[0].iteritems(), total=len(train_df[0])):\n",
    "        f.write(row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
