{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import URLs, untar_data, get_text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Loose Change\" is a thought-provoking little documentary that draws attenti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(inputs, targets, model, optim, criterion, zero_grad=False):\n",
    "    if zero_grad:\n",
    "        optim.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\" Transform CSV dataset intp Pytorch dataset \"\"\"\n",
    "    def __init__(self, dataframe=None, csv_file=None, root_dir=None, text_col=None, label_col=None):\n",
    "        if dataframe is None and csv_file is None:\n",
    "            raise ValueError(\"For initializing the dataset, you need to either pass a dataframe or a file name.\")\n",
    "        if dataframe is not None and (text_col is None or label_col is None):\n",
    "            raise ValueError(\"You need to pass `text_col` and `label_col` when passing a dataframe.\")\n",
    "        if csv_file:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "        if dataframe is not None:\n",
    "            self.df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx  = idx.tolist()\n",
    "        return self.df.loc[idx, self.text_col], self.df.loc[idx, self.label_col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dls, model, optim, criterion, epochs=10):\n",
    "    loss = 0.0\n",
    "    for epoch in epochs:\n",
    "        for inputs, targets in dls:\n",
    "            loss = train_one_epoch(inputs, targets, model, optim, criterion, zero_grad=True)\n",
    "        print(\"Epoch: {}\\t loss: {}\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M476FCVSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TokenEmbedding(vocab_size=1000, embed_size=512)\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = torch.optim.SGD(embedding_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/re1372/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "imdb_ds = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 25,000\tTest: 25,000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.&lt;br /&gt;&lt;br /&gt;But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "0  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... ...  \n",
       "1  Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home,...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = imdb_ds['train'].data.to_pandas()\n",
    "test_df = imdb_ds['test'].data.to_pandas()\n",
    "\n",
    "print('Train: {:,}\\tTest: {:,}'.format(train_df.shape[0], test_df.shape[0]))\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextDataset(train_df, text_col=\"text\", label_col='label')\n",
    "test_ds = TextDataset(test_df, text_col=\"text\", label_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!',\n",
       " 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=23)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000lines [00:01, 13296.30lines/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(map(tokenizer, iter(train_df.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)]) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel()>0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2719])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_process(iter(train_df.text[:10])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 6,767,187\n",
      "Test Data: 6,615,916\n"
     ]
    }
   ],
   "source": [
    "train_processed = data_process(iter(train_df.text))\n",
    "test_processed = data_process(iter(test_df.text))\n",
    "\n",
    "print('Train Data: {:,}'.format(train_processed.shape[0]))\n",
    "print('Test Data: {:,}'.format(test_processed.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36718lines [00:00, 51660.29lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,\n",
    "                                      iter(io.open(train_filepath,encoding=\"utf8\"))\n",
    "                                     ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "test_processed = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size, device='cpu'):\n",
    "    # Devide the dataset into batches with the size of `batch_size`\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data.narrow(0, 0, nbatch*batch_size)\n",
    "    # Evenly divide the data across the batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 40#96\n",
    "eval_batch_size = 24 #96\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "train_processed = batchify(train_processed, batch_size, device=device)\n",
    "test_processed = batchify(test_processed, batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 40 |\tTraining length of each batch: 169,179\n"
     ]
    }
   ],
   "source": [
    "print('Number of Batches: {:,} |\\tTraining length of each batch: {:,}'.format(train_processed.shape[1], train_processed.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, bptt):\n",
    "    seq_len = min(bptt, max(len(source)-1-i, 0))\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 40])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(train_processed, 105734, 35)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TransformerCustomModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninput, nhead, nhidd, nlayer, dropout=0.5):\n",
    "        super().__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninput, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninput, nhead, nhidd, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayer)\n",
    "        self.encoder = nn.Embedding(ntoken, ninput)\n",
    "        self.ninput = ninput\n",
    "        self.decoder = nn.Linear(ninput, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0,1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask==1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninput)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhidd = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayer = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerCustomModel(ntoken, emsize, nhead, nhidd, nlayer, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train(train_data, model, criterion, optimizer, bptt, device, ):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_processed.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_processed, i, bptt)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntoken), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source, bptt):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = eval_model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i, batch_size)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = eval_model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntoken)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 4833 batches | lr 5.00 | ms/batch 25.93 | loss  8.18 | ppl  3582.69\n",
      "| epoch   1 |   400/ 4833 batches | lr 5.00 | ms/batch 25.70 | loss  6.76 | ppl   860.42\n",
      "| epoch   1 |   600/ 4833 batches | lr 5.00 | ms/batch 25.68 | loss  6.43 | ppl   622.30\n",
      "| epoch   1 |   800/ 4833 batches | lr 5.00 | ms/batch 25.68 | loss  6.31 | ppl   548.56\n",
      "| epoch   1 |  1000/ 4833 batches | lr 5.00 | ms/batch 25.68 | loss  6.20 | ppl   490.62\n",
      "| epoch   1 |  1200/ 4833 batches | lr 5.00 | ms/batch 25.61 | loss  6.05 | ppl   424.06\n",
      "| epoch   1 |  1400/ 4833 batches | lr 5.00 | ms/batch 25.61 | loss  6.00 | ppl   402.23\n",
      "| epoch   1 |  1600/ 4833 batches | lr 5.00 | ms/batch 25.58 | loss  5.97 | ppl   391.25\n",
      "| epoch   1 |  1800/ 4833 batches | lr 5.00 | ms/batch 25.57 | loss  5.92 | ppl   371.03\n",
      "| epoch   1 |  2000/ 4833 batches | lr 5.00 | ms/batch 25.54 | loss  5.83 | ppl   338.85\n",
      "| epoch   1 |  2200/ 4833 batches | lr 5.00 | ms/batch 25.62 | loss  5.81 | ppl   333.34\n",
      "| epoch   1 |  2400/ 4833 batches | lr 5.00 | ms/batch 25.58 | loss  5.74 | ppl   311.30\n",
      "| epoch   1 |  2600/ 4833 batches | lr 5.00 | ms/batch 25.16 | loss  5.70 | ppl   298.84\n",
      "| epoch   1 |  2800/ 4833 batches | lr 5.00 | ms/batch 24.91 | loss  5.66 | ppl   287.25\n",
      "| epoch   1 |  3000/ 4833 batches | lr 5.00 | ms/batch 24.77 | loss  5.70 | ppl   300.18\n",
      "| epoch   1 |  3200/ 4833 batches | lr 5.00 | ms/batch 24.79 | loss  5.69 | ppl   297.37\n",
      "| epoch   1 |  3400/ 4833 batches | lr 5.00 | ms/batch 24.79 | loss  5.69 | ppl   295.52\n",
      "| epoch   1 |  3600/ 4833 batches | lr 5.00 | ms/batch 24.77 | loss  5.63 | ppl   278.08\n",
      "| epoch   1 |  3800/ 4833 batches | lr 5.00 | ms/batch 24.80 | loss  5.63 | ppl   277.91\n",
      "| epoch   1 |  4000/ 4833 batches | lr 5.00 | ms/batch 24.78 | loss  5.62 | ppl   275.17\n",
      "| epoch   1 |  4200/ 4833 batches | lr 5.00 | ms/batch 24.87 | loss  5.59 | ppl   266.83\n",
      "| epoch   1 |  4400/ 4833 batches | lr 5.00 | ms/batch 24.77 | loss  5.58 | ppl   264.54\n",
      "| epoch   1 |  4600/ 4833 batches | lr 5.00 | ms/batch 24.82 | loss  5.57 | ppl   263.22\n",
      "| epoch   1 |  4800/ 4833 batches | lr 5.00 | ms/batch 24.74 | loss  5.61 | ppl   274.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 161.95s | valid loss  6.35 | valid ppl   574.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 4833 batches | lr 4.51 | ms/batch 24.73 | loss  5.60 | ppl   269.54\n",
      "| epoch   2 |   400/ 4833 batches | lr 4.51 | ms/batch 24.65 | loss  5.49 | ppl   241.83\n",
      "| epoch   2 |   600/ 4833 batches | lr 4.51 | ms/batch 25.38 | loss  5.48 | ppl   240.76\n",
      "| epoch   2 |   800/ 4833 batches | lr 4.51 | ms/batch 25.47 | loss  5.53 | ppl   250.91\n",
      "| epoch   2 |  1000/ 4833 batches | lr 4.51 | ms/batch 25.49 | loss  5.51 | ppl   247.47\n",
      "| epoch   2 |  1200/ 4833 batches | lr 4.51 | ms/batch 25.48 | loss  5.45 | ppl   232.82\n",
      "| epoch   2 |  1400/ 4833 batches | lr 4.51 | ms/batch 25.44 | loss  5.45 | ppl   233.71\n",
      "| epoch   2 |  1600/ 4833 batches | lr 4.51 | ms/batch 25.53 | loss  5.49 | ppl   241.49\n",
      "| epoch   2 |  1800/ 4833 batches | lr 4.51 | ms/batch 25.47 | loss  5.46 | ppl   235.95\n",
      "| epoch   2 |  2000/ 4833 batches | lr 4.51 | ms/batch 25.47 | loss  5.42 | ppl   225.32\n",
      "| epoch   2 |  2200/ 4833 batches | lr 4.51 | ms/batch 25.45 | loss  5.41 | ppl   224.49\n",
      "| epoch   2 |  2400/ 4833 batches | lr 4.51 | ms/batch 25.48 | loss  5.38 | ppl   218.05\n",
      "| epoch   2 |  2600/ 4833 batches | lr 4.51 | ms/batch 25.58 | loss  5.37 | ppl   214.14\n",
      "| epoch   2 |  2800/ 4833 batches | lr 4.51 | ms/batch 25.51 | loss  5.36 | ppl   211.93\n",
      "| epoch   2 |  3000/ 4833 batches | lr 4.51 | ms/batch 25.44 | loss  5.41 | ppl   223.94\n",
      "| epoch   2 |  3200/ 4833 batches | lr 4.51 | ms/batch 25.43 | loss  5.41 | ppl   223.45\n",
      "| epoch   2 |  3400/ 4833 batches | lr 4.51 | ms/batch 25.43 | loss  5.42 | ppl   225.61\n",
      "| epoch   2 |  3600/ 4833 batches | lr 4.51 | ms/batch 25.45 | loss  5.37 | ppl   215.59\n",
      "| epoch   2 |  3800/ 4833 batches | lr 4.51 | ms/batch 25.46 | loss  5.38 | ppl   217.55\n",
      "| epoch   2 |  4000/ 4833 batches | lr 4.51 | ms/batch 25.45 | loss  5.39 | ppl   218.97\n",
      "| epoch   2 |  4200/ 4833 batches | lr 4.51 | ms/batch 25.52 | loss  5.36 | ppl   213.41\n",
      "| epoch   2 |  4400/ 4833 batches | lr 4.51 | ms/batch 25.43 | loss  5.37 | ppl   214.45\n",
      "| epoch   2 |  4600/ 4833 batches | lr 4.51 | ms/batch 25.44 | loss  5.37 | ppl   215.27\n",
      "| epoch   2 |  4800/ 4833 batches | lr 4.51 | ms/batch 25.70 | loss  5.41 | ppl   223.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 162.97s | valid loss  6.20 | valid ppl   492.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 4833 batches | lr 4.29 | ms/batch 25.58 | loss  5.41 | ppl   224.09\n",
      "| epoch   3 |   400/ 4833 batches | lr 4.29 | ms/batch 25.49 | loss  5.32 | ppl   204.23\n",
      "| epoch   3 |   600/ 4833 batches | lr 4.29 | ms/batch 25.51 | loss  5.33 | ppl   205.67\n",
      "| epoch   3 |   800/ 4833 batches | lr 4.29 | ms/batch 25.58 | loss  5.36 | ppl   213.56\n",
      "| epoch   3 |  1000/ 4833 batches | lr 4.29 | ms/batch 25.48 | loss  5.36 | ppl   212.59\n",
      "| epoch   3 |  1200/ 4833 batches | lr 4.29 | ms/batch 25.54 | loss  5.31 | ppl   201.46\n",
      "| epoch   3 |  1400/ 4833 batches | lr 4.29 | ms/batch 25.62 | loss  5.31 | ppl   202.03\n",
      "| epoch   3 |  1600/ 4833 batches | lr 4.29 | ms/batch 25.65 | loss  5.35 | ppl   210.02\n",
      "| epoch   3 |  1800/ 4833 batches | lr 4.29 | ms/batch 25.48 | loss  5.33 | ppl   206.53\n",
      "| epoch   3 |  2000/ 4833 batches | lr 4.29 | ms/batch 25.52 | loss  5.29 | ppl   197.81\n",
      "| epoch   3 |  2200/ 4833 batches | lr 4.29 | ms/batch 25.50 | loss  5.29 | ppl   198.43\n",
      "| epoch   3 |  2400/ 4833 batches | lr 4.29 | ms/batch 25.54 | loss  5.27 | ppl   193.49\n",
      "| epoch   3 |  2600/ 4833 batches | lr 4.29 | ms/batch 25.48 | loss  5.24 | ppl   189.19\n",
      "| epoch   3 |  2800/ 4833 batches | lr 4.29 | ms/batch 25.47 | loss  5.25 | ppl   189.70\n",
      "| epoch   3 |  3000/ 4833 batches | lr 4.29 | ms/batch 25.39 | loss  5.30 | ppl   199.91\n",
      "| epoch   3 |  3200/ 4833 batches | lr 4.29 | ms/batch 25.64 | loss  5.29 | ppl   198.89\n",
      "| epoch   3 |  3400/ 4833 batches | lr 4.29 | ms/batch 25.66 | loss  5.31 | ppl   202.00\n",
      "| epoch   3 |  3600/ 4833 batches | lr 4.29 | ms/batch 25.53 | loss  5.27 | ppl   193.86\n",
      "| epoch   3 |  3800/ 4833 batches | lr 4.29 | ms/batch 25.50 | loss  5.28 | ppl   196.52\n",
      "| epoch   3 |  4000/ 4833 batches | lr 4.29 | ms/batch 25.52 | loss  5.29 | ppl   198.62\n",
      "| epoch   3 |  4200/ 4833 batches | lr 4.29 | ms/batch 25.51 | loss  5.27 | ppl   193.71\n",
      "| epoch   3 |  4400/ 4833 batches | lr 4.29 | ms/batch 25.52 | loss  5.27 | ppl   193.95\n",
      "| epoch   3 |  4600/ 4833 batches | lr 4.29 | ms/batch 25.62 | loss  5.27 | ppl   195.20\n",
      "| epoch   3 |  4800/ 4833 batches | lr 4.29 | ms/batch 25.53 | loss  5.32 | ppl   203.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 163.72s | valid loss  6.08 | valid ppl   438.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 4833 batches | lr 4.07 | ms/batch 25.65 | loss  5.32 | ppl   205.03\n",
      "| epoch   4 |   400/ 4833 batches | lr 4.07 | ms/batch 25.53 | loss  5.23 | ppl   187.73\n",
      "| epoch   4 |   600/ 4833 batches | lr 4.07 | ms/batch 25.59 | loss  5.25 | ppl   189.70\n",
      "| epoch   4 |   800/ 4833 batches | lr 4.07 | ms/batch 25.54 | loss  5.28 | ppl   196.35\n",
      "| epoch   4 |  1000/ 4833 batches | lr 4.07 | ms/batch 25.57 | loss  5.28 | ppl   196.36\n",
      "| epoch   4 |  1200/ 4833 batches | lr 4.07 | ms/batch 25.59 | loss  5.23 | ppl   186.11\n",
      "| epoch   4 |  1400/ 4833 batches | lr 4.07 | ms/batch 25.55 | loss  5.23 | ppl   187.17\n",
      "| epoch   4 |  1600/ 4833 batches | lr 4.07 | ms/batch 25.57 | loss  5.27 | ppl   194.43\n",
      "| epoch   4 |  1800/ 4833 batches | lr 4.07 | ms/batch 25.64 | loss  5.25 | ppl   191.41\n",
      "| epoch   4 |  2000/ 4833 batches | lr 4.07 | ms/batch 25.60 | loss  5.21 | ppl   183.97\n",
      "| epoch   4 |  2200/ 4833 batches | lr 4.07 | ms/batch 25.60 | loss  5.22 | ppl   184.12\n",
      "| epoch   4 |  2400/ 4833 batches | lr 4.07 | ms/batch 25.58 | loss  5.19 | ppl   180.08\n",
      "| epoch   4 |  2600/ 4833 batches | lr 4.07 | ms/batch 25.59 | loss  5.18 | ppl   176.91\n",
      "| epoch   4 |  2800/ 4833 batches | lr 4.07 | ms/batch 25.58 | loss  5.17 | ppl   176.53\n",
      "| epoch   4 |  3000/ 4833 batches | lr 4.07 | ms/batch 25.66 | loss  5.22 | ppl   185.64\n",
      "| epoch   4 |  3200/ 4833 batches | lr 4.07 | ms/batch 25.71 | loss  5.22 | ppl   185.49\n",
      "| epoch   4 |  3400/ 4833 batches | lr 4.07 | ms/batch 25.60 | loss  5.24 | ppl   187.78\n",
      "| epoch   4 |  3600/ 4833 batches | lr 4.07 | ms/batch 25.62 | loss  5.20 | ppl   181.45\n",
      "| epoch   4 |  3800/ 4833 batches | lr 4.07 | ms/batch 25.60 | loss  5.21 | ppl   182.77\n",
      "| epoch   4 |  4000/ 4833 batches | lr 4.07 | ms/batch 25.57 | loss  5.22 | ppl   184.58\n",
      "| epoch   4 |  4200/ 4833 batches | lr 4.07 | ms/batch 25.59 | loss  5.20 | ppl   180.82\n",
      "| epoch   4 |  4400/ 4833 batches | lr 4.07 | ms/batch 25.57 | loss  5.20 | ppl   181.72\n",
      "| epoch   4 |  4600/ 4833 batches | lr 4.07 | ms/batch 25.57 | loss  5.20 | ppl   181.60\n",
      "| epoch   4 |  4800/ 4833 batches | lr 4.07 | ms/batch 25.59 | loss  5.24 | ppl   189.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 163.74s | valid loss  6.04 | valid ppl   417.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 4833 batches | lr 3.87 | ms/batch 25.70 | loss  5.25 | ppl   191.42\n",
      "| epoch   5 |   400/ 4833 batches | lr 3.87 | ms/batch 25.56 | loss  5.17 | ppl   176.52\n",
      "| epoch   5 |   600/ 4833 batches | lr 3.87 | ms/batch 25.57 | loss  5.18 | ppl   178.34\n",
      "| epoch   5 |   800/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.22 | ppl   184.22\n",
      "| epoch   5 |  1000/ 4833 batches | lr 3.87 | ms/batch 25.53 | loss  5.21 | ppl   183.78\n",
      "| epoch   5 |  1200/ 4833 batches | lr 3.87 | ms/batch 25.58 | loss  5.16 | ppl   175.00\n",
      "| epoch   5 |  1400/ 4833 batches | lr 3.87 | ms/batch 25.56 | loss  5.17 | ppl   175.70\n",
      "| epoch   5 |  1600/ 4833 batches | lr 3.87 | ms/batch 25.56 | loss  5.20 | ppl   181.90\n",
      "| epoch   5 |  1800/ 4833 batches | lr 3.87 | ms/batch 25.57 | loss  5.19 | ppl   179.66\n",
      "| epoch   5 |  2000/ 4833 batches | lr 3.87 | ms/batch 25.59 | loss  5.15 | ppl   172.62\n",
      "| epoch   5 |  2200/ 4833 batches | lr 3.87 | ms/batch 25.63 | loss  5.15 | ppl   172.51\n",
      "| epoch   5 |  2400/ 4833 batches | lr 3.87 | ms/batch 25.63 | loss  5.14 | ppl   170.03\n",
      "| epoch   5 |  2600/ 4833 batches | lr 3.87 | ms/batch 25.54 | loss  5.11 | ppl   165.96\n",
      "| epoch   5 |  2800/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.11 | ppl   165.86\n",
      "| epoch   5 |  3000/ 4833 batches | lr 3.87 | ms/batch 25.50 | loss  5.16 | ppl   174.21\n",
      "| epoch   5 |  3200/ 4833 batches | lr 3.87 | ms/batch 25.59 | loss  5.16 | ppl   173.92\n",
      "| epoch   5 |  3400/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.17 | ppl   176.18\n",
      "| epoch   5 |  3600/ 4833 batches | lr 3.87 | ms/batch 25.61 | loss  5.14 | ppl   170.30\n",
      "| epoch   5 |  3800/ 4833 batches | lr 3.87 | ms/batch 25.54 | loss  5.14 | ppl   170.86\n",
      "| epoch   5 |  4000/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.16 | ppl   173.33\n",
      "| epoch   5 |  4200/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.14 | ppl   170.13\n",
      "| epoch   5 |  4400/ 4833 batches | lr 3.87 | ms/batch 25.55 | loss  5.14 | ppl   170.58\n",
      "| epoch   5 |  4600/ 4833 batches | lr 3.87 | ms/batch 25.58 | loss  5.14 | ppl   170.89\n",
      "| epoch   5 |  4800/ 4833 batches | lr 3.87 | ms/batch 25.58 | loss  5.18 | ppl   177.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 163.53s | valid loss  6.00 | valid ppl   405.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 4833 batches | lr 3.68 | ms/batch 25.64 | loss  5.20 | ppl   180.43\n",
      "| epoch   6 |   400/ 4833 batches | lr 3.68 | ms/batch 25.51 | loss  5.12 | ppl   167.51\n",
      "| epoch   6 |   600/ 4833 batches | lr 3.68 | ms/batch 25.52 | loss  5.13 | ppl   168.60\n",
      "| epoch   6 |   800/ 4833 batches | lr 3.68 | ms/batch 25.46 | loss  5.16 | ppl   174.01\n",
      "| epoch   6 |  1000/ 4833 batches | lr 3.68 | ms/batch 25.47 | loss  5.16 | ppl   174.26\n",
      "| epoch   6 |  1200/ 4833 batches | lr 3.68 | ms/batch 25.46 | loss  5.11 | ppl   165.87\n",
      "| epoch   6 |  1400/ 4833 batches | lr 3.68 | ms/batch 25.48 | loss  5.12 | ppl   166.91\n",
      "| epoch   6 |  1600/ 4833 batches | lr 3.68 | ms/batch 25.48 | loss  5.15 | ppl   172.49\n",
      "| epoch   6 |  1800/ 4833 batches | lr 3.68 | ms/batch 25.48 | loss  5.14 | ppl   170.08\n",
      "| epoch   6 |  2000/ 4833 batches | lr 3.68 | ms/batch 25.51 | loss  5.10 | ppl   164.20\n",
      "| epoch   6 |  2200/ 4833 batches | lr 3.68 | ms/batch 25.49 | loss  5.10 | ppl   164.39\n",
      "| epoch   6 |  2400/ 4833 batches | lr 3.68 | ms/batch 25.56 | loss  5.08 | ppl   161.37\n",
      "| epoch   6 |  2600/ 4833 batches | lr 3.68 | ms/batch 25.52 | loss  5.06 | ppl   157.26\n",
      "| epoch   6 |  2800/ 4833 batches | lr 3.68 | ms/batch 25.50 | loss  5.07 | ppl   158.49\n",
      "| epoch   6 |  3000/ 4833 batches | lr 3.68 | ms/batch 25.56 | loss  5.11 | ppl   165.55\n",
      "| epoch   6 |  3200/ 4833 batches | lr 3.68 | ms/batch 25.55 | loss  5.11 | ppl   165.52\n",
      "| epoch   6 |  3400/ 4833 batches | lr 3.68 | ms/batch 25.50 | loss  5.12 | ppl   167.96\n",
      "| epoch   6 |  3600/ 4833 batches | lr 3.68 | ms/batch 25.53 | loss  5.09 | ppl   162.05\n",
      "| epoch   6 |  3800/ 4833 batches | lr 3.68 | ms/batch 25.56 | loss  5.10 | ppl   163.50\n",
      "| epoch   6 |  4000/ 4833 batches | lr 3.68 | ms/batch 25.60 | loss  5.11 | ppl   165.24\n",
      "| epoch   6 |  4200/ 4833 batches | lr 3.68 | ms/batch 25.59 | loss  5.09 | ppl   161.94\n",
      "| epoch   6 |  4400/ 4833 batches | lr 3.68 | ms/batch 25.49 | loss  5.09 | ppl   162.44\n",
      "| epoch   6 |  4600/ 4833 batches | lr 3.68 | ms/batch 25.53 | loss  5.09 | ppl   162.77\n",
      "| epoch   6 |  4800/ 4833 batches | lr 3.68 | ms/batch 25.53 | loss  5.13 | ppl   169.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 163.28s | valid loss  6.00 | valid ppl   403.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 4833 batches | lr 3.49 | ms/batch 25.70 | loss  5.15 | ppl   171.88\n",
      "| epoch   7 |   400/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.07 | ppl   159.71\n",
      "| epoch   7 |   600/ 4833 batches | lr 3.49 | ms/batch 25.54 | loss  5.09 | ppl   161.73\n",
      "| epoch   7 |   800/ 4833 batches | lr 3.49 | ms/batch 25.58 | loss  5.12 | ppl   166.64\n",
      "| epoch   7 |  1000/ 4833 batches | lr 3.49 | ms/batch 25.55 | loss  5.11 | ppl   166.50\n",
      "| epoch   7 |  1200/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.07 | ppl   158.57\n",
      "| epoch   7 |  1400/ 4833 batches | lr 3.49 | ms/batch 25.68 | loss  5.07 | ppl   159.52\n",
      "| epoch   7 |  1600/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.11 | ppl   165.21\n",
      "| epoch   7 |  1800/ 4833 batches | lr 3.49 | ms/batch 25.54 | loss  5.09 | ppl   162.65\n",
      "| epoch   7 |  2000/ 4833 batches | lr 3.49 | ms/batch 25.54 | loss  5.06 | ppl   157.19\n",
      "| epoch   7 |  2200/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.06 | ppl   157.58\n",
      "| epoch   7 |  2400/ 4833 batches | lr 3.49 | ms/batch 25.54 | loss  5.04 | ppl   154.39\n",
      "| epoch   7 |  2600/ 4833 batches | lr 3.49 | ms/batch 25.59 | loss  5.02 | ppl   151.33\n",
      "| epoch   7 |  2800/ 4833 batches | lr 3.49 | ms/batch 25.53 | loss  5.02 | ppl   151.84\n",
      "| epoch   7 |  3000/ 4833 batches | lr 3.49 | ms/batch 25.55 | loss  5.07 | ppl   158.96\n",
      "| epoch   7 |  3200/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.07 | ppl   159.14\n",
      "| epoch   7 |  3400/ 4833 batches | lr 3.49 | ms/batch 25.57 | loss  5.08 | ppl   160.87\n",
      "| epoch   7 |  3600/ 4833 batches | lr 3.49 | ms/batch 25.55 | loss  5.04 | ppl   155.07\n",
      "| epoch   7 |  3800/ 4833 batches | lr 3.49 | ms/batch 25.59 | loss  5.05 | ppl   156.78\n",
      "| epoch   7 |  4000/ 4833 batches | lr 3.49 | ms/batch 25.53 | loss  5.07 | ppl   158.85\n",
      "| epoch   7 |  4200/ 4833 batches | lr 3.49 | ms/batch 25.56 | loss  5.05 | ppl   155.38\n",
      "| epoch   7 |  4400/ 4833 batches | lr 3.49 | ms/batch 25.51 | loss  5.05 | ppl   156.23\n",
      "| epoch   7 |  4600/ 4833 batches | lr 3.49 | ms/batch 25.57 | loss  5.06 | ppl   156.81\n",
      "| epoch   7 |  4800/ 4833 batches | lr 3.49 | ms/batch 25.60 | loss  5.09 | ppl   161.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 163.51s | valid loss  6.00 | valid ppl   402.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 4833 batches | lr 3.32 | ms/batch 25.73 | loss  5.10 | ppl   164.76\n",
      "| epoch   8 |   400/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.04 | ppl   153.97\n",
      "| epoch   8 |   600/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.04 | ppl   154.81\n",
      "| epoch   8 |   800/ 4833 batches | lr 3.32 | ms/batch 25.64 | loss  5.08 | ppl   160.09\n",
      "| epoch   8 |  1000/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.08 | ppl   160.45\n",
      "| epoch   8 |  1200/ 4833 batches | lr 3.32 | ms/batch 25.54 | loss  5.03 | ppl   152.97\n",
      "| epoch   8 |  1400/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.04 | ppl   154.00\n",
      "| epoch   8 |  1600/ 4833 batches | lr 3.32 | ms/batch 25.57 | loss  5.07 | ppl   158.74\n",
      "| epoch   8 |  1800/ 4833 batches | lr 3.32 | ms/batch 25.61 | loss  5.06 | ppl   156.85\n",
      "| epoch   8 |  2000/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.02 | ppl   151.77\n",
      "| epoch   8 |  2200/ 4833 batches | lr 3.32 | ms/batch 25.55 | loss  5.02 | ppl   151.99\n",
      "| epoch   8 |  2400/ 4833 batches | lr 3.32 | ms/batch 25.50 | loss  5.00 | ppl   148.61\n",
      "| epoch   8 |  2600/ 4833 batches | lr 3.32 | ms/batch 25.49 | loss  4.98 | ppl   145.81\n",
      "| epoch   8 |  2800/ 4833 batches | lr 3.32 | ms/batch 25.50 | loss  4.98 | ppl   146.00\n",
      "| epoch   8 |  3000/ 4833 batches | lr 3.32 | ms/batch 25.50 | loss  5.03 | ppl   152.88\n",
      "| epoch   8 |  3200/ 4833 batches | lr 3.32 | ms/batch 25.59 | loss  5.03 | ppl   152.74\n",
      "| epoch   8 |  3400/ 4833 batches | lr 3.32 | ms/batch 25.52 | loss  5.04 | ppl   154.33\n",
      "| epoch   8 |  3600/ 4833 batches | lr 3.32 | ms/batch 25.51 | loss  5.01 | ppl   149.64\n",
      "| epoch   8 |  3800/ 4833 batches | lr 3.32 | ms/batch 25.55 | loss  5.02 | ppl   150.83\n",
      "| epoch   8 |  4000/ 4833 batches | lr 3.32 | ms/batch 25.52 | loss  5.03 | ppl   152.78\n",
      "| epoch   8 |  4200/ 4833 batches | lr 3.32 | ms/batch 25.54 | loss  5.01 | ppl   150.00\n",
      "| epoch   8 |  4400/ 4833 batches | lr 3.32 | ms/batch 25.60 | loss  5.01 | ppl   150.29\n",
      "| epoch   8 |  4600/ 4833 batches | lr 3.32 | ms/batch 25.54 | loss  5.02 | ppl   151.07\n",
      "| epoch   8 |  4800/ 4833 batches | lr 3.32 | ms/batch 25.56 | loss  5.05 | ppl   156.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 163.49s | valid loss  5.98 | valid ppl   394.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 4833 batches | lr 3.15 | ms/batch 25.72 | loss  5.07 | ppl   159.13\n",
      "| epoch   9 |   400/ 4833 batches | lr 3.15 | ms/batch 25.57 | loss  5.00 | ppl   148.40\n",
      "| epoch   9 |   600/ 4833 batches | lr 3.15 | ms/batch 25.53 | loss  5.01 | ppl   149.73\n",
      "| epoch   9 |   800/ 4833 batches | lr 3.15 | ms/batch 25.57 | loss  5.04 | ppl   154.74\n",
      "| epoch   9 |  1000/ 4833 batches | lr 3.15 | ms/batch 25.55 | loss  5.04 | ppl   154.76\n",
      "| epoch   9 |  1200/ 4833 batches | lr 3.15 | ms/batch 25.50 | loss  4.99 | ppl   147.60\n",
      "| epoch   9 |  1400/ 4833 batches | lr 3.15 | ms/batch 25.60 | loss  5.00 | ppl   148.55\n",
      "| epoch   9 |  1600/ 4833 batches | lr 3.15 | ms/batch 25.55 | loss  5.03 | ppl   152.95\n",
      "| epoch   9 |  1800/ 4833 batches | lr 3.15 | ms/batch 25.54 | loss  5.02 | ppl   151.47\n",
      "| epoch   9 |  2000/ 4833 batches | lr 3.15 | ms/batch 25.52 | loss  4.99 | ppl   146.50\n",
      "| epoch   9 |  2200/ 4833 batches | lr 3.15 | ms/batch 25.53 | loss  4.99 | ppl   146.37\n",
      "| epoch   9 |  2400/ 4833 batches | lr 3.15 | ms/batch 25.53 | loss  4.97 | ppl   143.68\n",
      "| epoch   9 |  2600/ 4833 batches | lr 3.15 | ms/batch 25.54 | loss  4.95 | ppl   140.99\n",
      "| epoch   9 |  2800/ 4833 batches | lr 3.15 | ms/batch 25.54 | loss  4.95 | ppl   141.31\n",
      "| epoch   9 |  3000/ 4833 batches | lr 3.15 | ms/batch 25.50 | loss  5.00 | ppl   147.72\n",
      "| epoch   9 |  3200/ 4833 batches | lr 3.15 | ms/batch 25.53 | loss  4.99 | ppl   146.86\n",
      "| epoch   9 |  3400/ 4833 batches | lr 3.15 | ms/batch 25.55 | loss  5.00 | ppl   149.10\n",
      "| epoch   9 |  3600/ 4833 batches | lr 3.15 | ms/batch 25.52 | loss  4.97 | ppl   144.09\n",
      "| epoch   9 |  3800/ 4833 batches | lr 3.15 | ms/batch 25.62 | loss  4.98 | ppl   145.39\n",
      "| epoch   9 |  4000/ 4833 batches | lr 3.15 | ms/batch 25.54 | loss  4.99 | ppl   147.33\n",
      "| epoch   9 |  4200/ 4833 batches | lr 3.15 | ms/batch 25.51 | loss  4.98 | ppl   144.94\n",
      "| epoch   9 |  4400/ 4833 batches | lr 3.15 | ms/batch 25.54 | loss  4.98 | ppl   145.24\n",
      "| epoch   9 |  4600/ 4833 batches | lr 3.15 | ms/batch 25.57 | loss  4.98 | ppl   146.07\n",
      "| epoch   9 |  4800/ 4833 batches | lr 3.15 | ms/batch 25.62 | loss  5.01 | ppl   150.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 163.49s | valid loss  5.97 | valid ppl   392.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 4833 batches | lr 2.99 | ms/batch 25.77 | loss  5.03 | ppl   152.93\n",
      "| epoch  10 |   400/ 4833 batches | lr 2.99 | ms/batch 25.50 | loss  4.97 | ppl   143.56\n",
      "| epoch  10 |   600/ 4833 batches | lr 2.99 | ms/batch 25.51 | loss  4.98 | ppl   145.16\n",
      "| epoch  10 |   800/ 4833 batches | lr 2.99 | ms/batch 25.51 | loss  5.00 | ppl   149.15\n",
      "| epoch  10 |  1000/ 4833 batches | lr 2.99 | ms/batch 25.55 | loss  5.01 | ppl   149.49\n",
      "| epoch  10 |  1200/ 4833 batches | lr 2.99 | ms/batch 25.49 | loss  4.96 | ppl   142.58\n",
      "| epoch  10 |  1400/ 4833 batches | lr 2.99 | ms/batch 25.52 | loss  4.97 | ppl   143.85\n",
      "| epoch  10 |  1600/ 4833 batches | lr 2.99 | ms/batch 25.61 | loss  4.99 | ppl   147.55\n",
      "| epoch  10 |  1800/ 4833 batches | lr 2.99 | ms/batch 25.52 | loss  4.99 | ppl   146.42\n",
      "| epoch  10 |  2000/ 4833 batches | lr 2.99 | ms/batch 25.49 | loss  4.96 | ppl   142.13\n",
      "| epoch  10 |  2200/ 4833 batches | lr 2.99 | ms/batch 25.59 | loss  4.95 | ppl   141.57\n",
      "| epoch  10 |  2400/ 4833 batches | lr 2.99 | ms/batch 25.53 | loss  4.94 | ppl   139.57\n",
      "| epoch  10 |  2600/ 4833 batches | lr 2.99 | ms/batch 25.49 | loss  4.92 | ppl   136.77\n",
      "| epoch  10 |  2800/ 4833 batches | lr 2.99 | ms/batch 25.53 | loss  4.91 | ppl   136.26\n",
      "| epoch  10 |  3000/ 4833 batches | lr 2.99 | ms/batch 25.58 | loss  4.96 | ppl   142.92\n",
      "| epoch  10 |  3200/ 4833 batches | lr 2.99 | ms/batch 25.60 | loss  4.96 | ppl   142.36\n",
      "| epoch  10 |  3400/ 4833 batches | lr 2.99 | ms/batch 25.59 | loss  4.97 | ppl   144.22\n",
      "| epoch  10 |  3600/ 4833 batches | lr 2.99 | ms/batch 25.60 | loss  4.94 | ppl   139.31\n",
      "| epoch  10 |  3800/ 4833 batches | lr 2.99 | ms/batch 25.58 | loss  4.95 | ppl   140.89\n",
      "| epoch  10 |  4000/ 4833 batches | lr 2.99 | ms/batch 25.60 | loss  4.96 | ppl   143.02\n",
      "| epoch  10 |  4200/ 4833 batches | lr 2.99 | ms/batch 25.54 | loss  4.94 | ppl   140.33\n",
      "| epoch  10 |  4400/ 4833 batches | lr 2.99 | ms/batch 25.57 | loss  4.95 | ppl   141.33\n",
      "| epoch  10 |  4600/ 4833 batches | lr 2.99 | ms/batch 25.58 | loss  4.95 | ppl   141.30\n",
      "| epoch  10 |  4800/ 4833 batches | lr 2.99 | ms/batch 25.60 | loss  4.98 | ppl   145.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 163.42s | valid loss  5.98 | valid ppl   393.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 4833 batches | lr 2.84 | ms/batch 25.82 | loss  5.00 | ppl   148.72\n",
      "| epoch  11 |   400/ 4833 batches | lr 2.84 | ms/batch 25.60 | loss  4.93 | ppl   138.88\n",
      "| epoch  11 |   600/ 4833 batches | lr 2.84 | ms/batch 25.59 | loss  4.94 | ppl   140.24\n",
      "| epoch  11 |   800/ 4833 batches | lr 2.84 | ms/batch 25.63 | loss  4.97 | ppl   144.50\n",
      "| epoch  11 |  1000/ 4833 batches | lr 2.84 | ms/batch 25.58 | loss  4.98 | ppl   145.06\n",
      "| epoch  11 |  1200/ 4833 batches | lr 2.84 | ms/batch 25.62 | loss  4.93 | ppl   138.39\n",
      "| epoch  11 |  1400/ 4833 batches | lr 2.84 | ms/batch 25.68 | loss  4.94 | ppl   139.70\n",
      "| epoch  11 |  1600/ 4833 batches | lr 2.84 | ms/batch 25.64 | loss  4.96 | ppl   143.02\n",
      "| epoch  11 |  1800/ 4833 batches | lr 2.84 | ms/batch 25.66 | loss  4.96 | ppl   142.48\n",
      "| epoch  11 |  2000/ 4833 batches | lr 2.84 | ms/batch 25.59 | loss  4.92 | ppl   137.53\n",
      "| epoch  11 |  2200/ 4833 batches | lr 2.84 | ms/batch 25.63 | loss  4.92 | ppl   137.48\n",
      "| epoch  11 |  2400/ 4833 batches | lr 2.84 | ms/batch 25.66 | loss  4.91 | ppl   134.98\n",
      "| epoch  11 |  2600/ 4833 batches | lr 2.84 | ms/batch 25.59 | loss  4.88 | ppl   132.27\n",
      "| epoch  11 |  2800/ 4833 batches | lr 2.84 | ms/batch 25.73 | loss  4.89 | ppl   132.43\n",
      "| epoch  11 |  3000/ 4833 batches | lr 2.84 | ms/batch 25.62 | loss  4.93 | ppl   138.46\n",
      "| epoch  11 |  3200/ 4833 batches | lr 2.84 | ms/batch 25.64 | loss  4.93 | ppl   138.66\n",
      "| epoch  11 |  3400/ 4833 batches | lr 2.84 | ms/batch 25.63 | loss  4.94 | ppl   140.12\n",
      "| epoch  11 |  3600/ 4833 batches | lr 2.84 | ms/batch 25.64 | loss  4.91 | ppl   135.53\n",
      "| epoch  11 |  3800/ 4833 batches | lr 2.84 | ms/batch 25.63 | loss  4.92 | ppl   137.04\n",
      "| epoch  11 |  4000/ 4833 batches | lr 2.84 | ms/batch 25.71 | loss  4.93 | ppl   138.39\n",
      "| epoch  11 |  4200/ 4833 batches | lr 2.84 | ms/batch 25.65 | loss  4.91 | ppl   136.17\n",
      "| epoch  11 |  4400/ 4833 batches | lr 2.84 | ms/batch 25.66 | loss  4.92 | ppl   136.84\n",
      "| epoch  11 |  4600/ 4833 batches | lr 2.84 | ms/batch 25.64 | loss  4.92 | ppl   137.31\n",
      "| epoch  11 |  4800/ 4833 batches | lr 2.84 | ms/batch 25.65 | loss  4.95 | ppl   141.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 163.94s | valid loss  5.98 | valid ppl   394.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 4833 batches | lr 2.70 | ms/batch 25.86 | loss  4.97 | ppl   144.29\n",
      "| epoch  12 |   400/ 4833 batches | lr 2.70 | ms/batch 25.61 | loss  4.91 | ppl   135.06\n",
      "| epoch  12 |   600/ 4833 batches | lr 2.70 | ms/batch 25.61 | loss  4.91 | ppl   136.07\n",
      "| epoch  12 |   800/ 4833 batches | lr 2.70 | ms/batch 25.68 | loss  4.94 | ppl   140.15\n",
      "| epoch  12 |  1000/ 4833 batches | lr 2.70 | ms/batch 25.72 | loss  4.95 | ppl   140.96\n",
      "| epoch  12 |  1200/ 4833 batches | lr 2.70 | ms/batch 25.58 | loss  4.90 | ppl   134.68\n",
      "| epoch  12 |  1400/ 4833 batches | lr 2.70 | ms/batch 25.64 | loss  4.91 | ppl   135.66\n",
      "| epoch  12 |  1600/ 4833 batches | lr 2.70 | ms/batch 25.69 | loss  4.93 | ppl   139.00\n",
      "| epoch  12 |  1800/ 4833 batches | lr 2.70 | ms/batch 25.68 | loss  4.93 | ppl   138.23\n",
      "| epoch  12 |  2000/ 4833 batches | lr 2.70 | ms/batch 25.63 | loss  4.90 | ppl   134.49\n",
      "| epoch  12 |  2200/ 4833 batches | lr 2.70 | ms/batch 25.75 | loss  4.90 | ppl   133.70\n",
      "| epoch  12 |  2400/ 4833 batches | lr 2.70 | ms/batch 25.69 | loss  4.88 | ppl   131.26\n",
      "| epoch  12 |  2600/ 4833 batches | lr 2.70 | ms/batch 25.68 | loss  4.86 | ppl   128.89\n",
      "| epoch  12 |  2800/ 4833 batches | lr 2.70 | ms/batch 25.66 | loss  4.86 | ppl   129.20\n",
      "| epoch  12 |  3000/ 4833 batches | lr 2.70 | ms/batch 25.66 | loss  4.90 | ppl   134.41\n",
      "| epoch  12 |  3200/ 4833 batches | lr 2.70 | ms/batch 25.70 | loss  4.90 | ppl   134.81\n",
      "| epoch  12 |  3400/ 4833 batches | lr 2.70 | ms/batch 25.71 | loss  4.91 | ppl   136.26\n",
      "| epoch  12 |  3600/ 4833 batches | lr 2.70 | ms/batch 25.70 | loss  4.88 | ppl   131.86\n",
      "| epoch  12 |  3800/ 4833 batches | lr 2.70 | ms/batch 25.72 | loss  4.89 | ppl   133.17\n",
      "| epoch  12 |  4000/ 4833 batches | lr 2.70 | ms/batch 25.69 | loss  4.91 | ppl   135.07\n",
      "| epoch  12 |  4200/ 4833 batches | lr 2.70 | ms/batch 25.73 | loss  4.89 | ppl   132.33\n",
      "| epoch  12 |  4400/ 4833 batches | lr 2.70 | ms/batch 25.74 | loss  4.89 | ppl   133.25\n",
      "| epoch  12 |  4600/ 4833 batches | lr 2.70 | ms/batch 25.79 | loss  4.89 | ppl   133.19\n",
      "| epoch  12 |  4800/ 4833 batches | lr 2.70 | ms/batch 25.74 | loss  4.92 | ppl   137.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 164.23s | valid loss  5.98 | valid ppl   395.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 4833 batches | lr 2.57 | ms/batch 25.82 | loss  4.94 | ppl   140.12\n",
      "| epoch  13 |   400/ 4833 batches | lr 2.57 | ms/batch 25.66 | loss  4.88 | ppl   131.51\n",
      "| epoch  13 |   600/ 4833 batches | lr 2.57 | ms/batch 25.68 | loss  4.89 | ppl   133.17\n",
      "| epoch  13 |   800/ 4833 batches | lr 2.57 | ms/batch 25.67 | loss  4.92 | ppl   136.61\n",
      "| epoch  13 |  1000/ 4833 batches | lr 2.57 | ms/batch 25.69 | loss  4.92 | ppl   137.35\n",
      "| epoch  13 |  1200/ 4833 batches | lr 2.57 | ms/batch 25.66 | loss  4.87 | ppl   130.78\n",
      "| epoch  13 |  1400/ 4833 batches | lr 2.57 | ms/batch 25.71 | loss  4.88 | ppl   132.14\n",
      "| epoch  13 |  1600/ 4833 batches | lr 2.57 | ms/batch 25.76 | loss  4.91 | ppl   135.23\n",
      "| epoch  13 |  1800/ 4833 batches | lr 2.57 | ms/batch 25.70 | loss  4.90 | ppl   134.52\n",
      "| epoch  13 |  2000/ 4833 batches | lr 2.57 | ms/batch 25.67 | loss  4.87 | ppl   130.89\n",
      "| epoch  13 |  2200/ 4833 batches | lr 2.57 | ms/batch 25.64 | loss  4.87 | ppl   130.49\n",
      "| epoch  13 |  2400/ 4833 batches | lr 2.57 | ms/batch 25.71 | loss  4.85 | ppl   127.97\n",
      "| epoch  13 |  2600/ 4833 batches | lr 2.57 | ms/batch 25.73 | loss  4.84 | ppl   125.85\n",
      "| epoch  13 |  2800/ 4833 batches | lr 2.57 | ms/batch 25.76 | loss  4.83 | ppl   125.58\n",
      "| epoch  13 |  3000/ 4833 batches | lr 2.57 | ms/batch 25.70 | loss  4.87 | ppl   130.83\n",
      "| epoch  13 |  3200/ 4833 batches | lr 2.57 | ms/batch 25.69 | loss  4.87 | ppl   130.91\n",
      "| epoch  13 |  3400/ 4833 batches | lr 2.57 | ms/batch 25.71 | loss  4.89 | ppl   132.75\n",
      "| epoch  13 |  3600/ 4833 batches | lr 2.57 | ms/batch 25.69 | loss  4.86 | ppl   128.60\n",
      "| epoch  13 |  3800/ 4833 batches | lr 2.57 | ms/batch 25.69 | loss  4.87 | ppl   129.75\n",
      "| epoch  13 |  4000/ 4833 batches | lr 2.57 | ms/batch 25.71 | loss  4.88 | ppl   131.28\n",
      "| epoch  13 |  4200/ 4833 batches | lr 2.57 | ms/batch 25.77 | loss  4.86 | ppl   128.52\n",
      "| epoch  13 |  4400/ 4833 batches | lr 2.57 | ms/batch 25.73 | loss  4.87 | ppl   129.87\n",
      "| epoch  13 |  4600/ 4833 batches | lr 2.57 | ms/batch 25.69 | loss  4.87 | ppl   130.45\n",
      "| epoch  13 |  4800/ 4833 batches | lr 2.57 | ms/batch 25.72 | loss  4.90 | ppl   133.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 164.15s | valid loss  5.98 | valid ppl   394.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 4833 batches | lr 2.44 | ms/batch 25.86 | loss  4.92 | ppl   136.69\n",
      "| epoch  14 |   400/ 4833 batches | lr 2.44 | ms/batch 25.71 | loss  4.85 | ppl   128.33\n",
      "| epoch  14 |   600/ 4833 batches | lr 2.44 | ms/batch 25.71 | loss  4.87 | ppl   129.78\n",
      "| epoch  14 |   800/ 4833 batches | lr 2.44 | ms/batch 25.78 | loss  4.89 | ppl   133.09\n",
      "| epoch  14 |  1000/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.90 | ppl   133.66\n",
      "| epoch  14 |  1200/ 4833 batches | lr 2.44 | ms/batch 25.76 | loss  4.85 | ppl   127.86\n",
      "| epoch  14 |  1400/ 4833 batches | lr 2.44 | ms/batch 25.74 | loss  4.86 | ppl   128.60\n",
      "| epoch  14 |  1600/ 4833 batches | lr 2.44 | ms/batch 25.75 | loss  4.88 | ppl   131.97\n",
      "| epoch  14 |  1800/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.88 | ppl   131.36\n",
      "| epoch  14 |  2000/ 4833 batches | lr 2.44 | ms/batch 25.70 | loss  4.85 | ppl   127.65\n",
      "| epoch  14 |  2200/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.85 | ppl   127.11\n",
      "| epoch  14 |  2400/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.83 | ppl   124.97\n",
      "| epoch  14 |  2600/ 4833 batches | lr 2.44 | ms/batch 25.67 | loss  4.81 | ppl   122.87\n",
      "| epoch  14 |  2800/ 4833 batches | lr 2.44 | ms/batch 25.71 | loss  4.81 | ppl   122.83\n",
      "| epoch  14 |  3000/ 4833 batches | lr 2.44 | ms/batch 25.75 | loss  4.85 | ppl   127.63\n",
      "| epoch  14 |  3200/ 4833 batches | lr 2.44 | ms/batch 25.75 | loss  4.85 | ppl   128.07\n",
      "| epoch  14 |  3400/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.87 | ppl   129.68\n",
      "| epoch  14 |  3600/ 4833 batches | lr 2.44 | ms/batch 25.76 | loss  4.83 | ppl   125.63\n",
      "| epoch  14 |  3800/ 4833 batches | lr 2.44 | ms/batch 25.75 | loss  4.84 | ppl   126.61\n",
      "| epoch  14 |  4000/ 4833 batches | lr 2.44 | ms/batch 25.72 | loss  4.85 | ppl   128.15\n",
      "| epoch  14 |  4200/ 4833 batches | lr 2.44 | ms/batch 25.70 | loss  4.83 | ppl   125.82\n",
      "| epoch  14 |  4400/ 4833 batches | lr 2.44 | ms/batch 25.71 | loss  4.84 | ppl   126.69\n",
      "| epoch  14 |  4600/ 4833 batches | lr 2.44 | ms/batch 25.76 | loss  4.85 | ppl   127.20\n",
      "| epoch  14 |  4800/ 4833 batches | lr 2.44 | ms/batch 25.77 | loss  4.87 | ppl   130.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 164.38s | valid loss  5.99 | valid ppl   399.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 4833 batches | lr 2.32 | ms/batch 25.92 | loss  4.89 | ppl   133.31\n",
      "| epoch  15 |   400/ 4833 batches | lr 2.32 | ms/batch 25.76 | loss  4.83 | ppl   125.40\n",
      "| epoch  15 |   600/ 4833 batches | lr 2.32 | ms/batch 25.79 | loss  4.84 | ppl   126.06\n",
      "| epoch  15 |   800/ 4833 batches | lr 2.32 | ms/batch 25.76 | loss  4.87 | ppl   129.88\n",
      "| epoch  15 |  1000/ 4833 batches | lr 2.32 | ms/batch 25.79 | loss  4.87 | ppl   130.82\n",
      "| epoch  15 |  1200/ 4833 batches | lr 2.32 | ms/batch 25.78 | loss  4.83 | ppl   125.06\n",
      "| epoch  15 |  1400/ 4833 batches | lr 2.32 | ms/batch 25.82 | loss  4.83 | ppl   125.79\n",
      "| epoch  15 |  1600/ 4833 batches | lr 2.32 | ms/batch 25.81 | loss  4.86 | ppl   128.96\n",
      "| epoch  15 |  1800/ 4833 batches | lr 2.32 | ms/batch 25.85 | loss  4.85 | ppl   127.93\n",
      "| epoch  15 |  2000/ 4833 batches | lr 2.32 | ms/batch 25.80 | loss  4.83 | ppl   124.61\n",
      "| epoch  15 |  2200/ 4833 batches | lr 2.32 | ms/batch 25.82 | loss  4.82 | ppl   124.12\n",
      "| epoch  15 |  2400/ 4833 batches | lr 2.32 | ms/batch 25.80 | loss  4.81 | ppl   122.20\n",
      "| epoch  15 |  2600/ 4833 batches | lr 2.32 | ms/batch 25.80 | loss  4.79 | ppl   119.84\n",
      "| epoch  15 |  2800/ 4833 batches | lr 2.32 | ms/batch 25.83 | loss  4.78 | ppl   119.36\n",
      "| epoch  15 |  3000/ 4833 batches | lr 2.32 | ms/batch 25.83 | loss  4.83 | ppl   124.61\n",
      "| epoch  15 |  3200/ 4833 batches | lr 2.32 | ms/batch 25.91 | loss  4.83 | ppl   124.72\n",
      "| epoch  15 |  3400/ 4833 batches | lr 2.32 | ms/batch 25.81 | loss  4.84 | ppl   126.28\n",
      "| epoch  15 |  3600/ 4833 batches | lr 2.32 | ms/batch 25.81 | loss  4.81 | ppl   122.54\n",
      "| epoch  15 |  3800/ 4833 batches | lr 2.32 | ms/batch 25.87 | loss  4.81 | ppl   123.25\n",
      "| epoch  15 |  4000/ 4833 batches | lr 2.32 | ms/batch 25.86 | loss  4.83 | ppl   125.05\n",
      "| epoch  15 |  4200/ 4833 batches | lr 2.32 | ms/batch 25.80 | loss  4.81 | ppl   122.67\n",
      "| epoch  15 |  4400/ 4833 batches | lr 2.32 | ms/batch 25.80 | loss  4.82 | ppl   123.71\n",
      "| epoch  15 |  4600/ 4833 batches | lr 2.32 | ms/batch 25.82 | loss  4.82 | ppl   124.23\n",
      "| epoch  15 |  4800/ 4833 batches | lr 2.32 | ms/batch 25.82 | loss  4.85 | ppl   127.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 164.84s | valid loss  5.97 | valid ppl   393.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 4833 batches | lr 2.20 | ms/batch 25.98 | loss  4.87 | ppl   129.87\n",
      "| epoch  16 |   400/ 4833 batches | lr 2.20 | ms/batch 25.80 | loss  4.81 | ppl   122.79\n",
      "| epoch  16 |   600/ 4833 batches | lr 2.20 | ms/batch 25.83 | loss  4.82 | ppl   123.39\n",
      "| epoch  16 |   800/ 4833 batches | lr 2.20 | ms/batch 25.85 | loss  4.84 | ppl   126.60\n",
      "| epoch  16 |  1000/ 4833 batches | lr 2.20 | ms/batch 25.83 | loss  4.85 | ppl   127.98\n",
      "| epoch  16 |  1200/ 4833 batches | lr 2.20 | ms/batch 25.86 | loss  4.80 | ppl   122.07\n",
      "| epoch  16 |  1400/ 4833 batches | lr 2.20 | ms/batch 25.83 | loss  4.81 | ppl   122.83\n",
      "| epoch  16 |  1600/ 4833 batches | lr 2.20 | ms/batch 25.84 | loss  4.83 | ppl   125.61\n",
      "| epoch  16 |  1800/ 4833 batches | lr 2.20 | ms/batch 25.85 | loss  4.83 | ppl   125.33\n",
      "| epoch  16 |  2000/ 4833 batches | lr 2.20 | ms/batch 25.88 | loss  4.80 | ppl   121.91\n",
      "| epoch  16 |  2200/ 4833 batches | lr 2.20 | ms/batch 25.88 | loss  4.80 | ppl   121.19\n",
      "| epoch  16 |  2400/ 4833 batches | lr 2.20 | ms/batch 25.85 | loss  4.78 | ppl   119.57\n",
      "| epoch  16 |  2600/ 4833 batches | lr 2.20 | ms/batch 25.82 | loss  4.76 | ppl   117.16\n",
      "| epoch  16 |  2800/ 4833 batches | lr 2.20 | ms/batch 25.82 | loss  4.76 | ppl   117.11\n",
      "| epoch  16 |  3000/ 4833 batches | lr 2.20 | ms/batch 25.83 | loss  4.80 | ppl   121.90\n",
      "| epoch  16 |  3200/ 4833 batches | lr 2.20 | ms/batch 25.77 | loss  4.81 | ppl   122.24\n",
      "| epoch  16 |  3400/ 4833 batches | lr 2.20 | ms/batch 25.86 | loss  4.81 | ppl   123.35\n",
      "| epoch  16 |  3600/ 4833 batches | lr 2.20 | ms/batch 25.85 | loss  4.78 | ppl   119.66\n",
      "| epoch  16 |  3800/ 4833 batches | lr 2.20 | ms/batch 25.84 | loss  4.79 | ppl   120.58\n",
      "| epoch  16 |  4000/ 4833 batches | lr 2.20 | ms/batch 25.82 | loss  4.81 | ppl   122.41\n",
      "| epoch  16 |  4200/ 4833 batches | lr 2.20 | ms/batch 25.84 | loss  4.79 | ppl   120.04\n",
      "| epoch  16 |  4400/ 4833 batches | lr 2.20 | ms/batch 25.82 | loss  4.80 | ppl   121.21\n",
      "| epoch  16 |  4600/ 4833 batches | lr 2.20 | ms/batch 25.89 | loss  4.80 | ppl   121.70\n",
      "| epoch  16 |  4800/ 4833 batches | lr 2.20 | ms/batch 25.87 | loss  4.82 | ppl   124.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 164.90s | valid loss  5.99 | valid ppl   401.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 4833 batches | lr 2.09 | ms/batch 26.01 | loss  4.85 | ppl   127.15\n",
      "| epoch  17 |   400/ 4833 batches | lr 2.09 | ms/batch 25.88 | loss  4.79 | ppl   119.82\n",
      "| epoch  17 |   600/ 4833 batches | lr 2.09 | ms/batch 25.90 | loss  4.79 | ppl   120.74\n",
      "| epoch  17 |   800/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.82 | ppl   123.62\n",
      "| epoch  17 |  1000/ 4833 batches | lr 2.09 | ms/batch 25.83 | loss  4.83 | ppl   124.68\n",
      "| epoch  17 |  1200/ 4833 batches | lr 2.09 | ms/batch 25.81 | loss  4.78 | ppl   119.51\n",
      "| epoch  17 |  1400/ 4833 batches | lr 2.09 | ms/batch 25.83 | loss  4.79 | ppl   120.53\n",
      "| epoch  17 |  1600/ 4833 batches | lr 2.09 | ms/batch 25.83 | loss  4.81 | ppl   123.00\n",
      "| epoch  17 |  1800/ 4833 batches | lr 2.09 | ms/batch 25.88 | loss  4.81 | ppl   122.71\n",
      "| epoch  17 |  2000/ 4833 batches | lr 2.09 | ms/batch 25.89 | loss  4.78 | ppl   119.27\n",
      "| epoch  17 |  2200/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.78 | ppl   118.87\n",
      "| epoch  17 |  2400/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.76 | ppl   117.05\n",
      "| epoch  17 |  2600/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.74 | ppl   114.82\n",
      "| epoch  17 |  2800/ 4833 batches | lr 2.09 | ms/batch 25.86 | loss  4.74 | ppl   114.78\n",
      "| epoch  17 |  3000/ 4833 batches | lr 2.09 | ms/batch 25.86 | loss  4.78 | ppl   119.18\n",
      "| epoch  17 |  3200/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.78 | ppl   119.50\n",
      "| epoch  17 |  3400/ 4833 batches | lr 2.09 | ms/batch 25.85 | loss  4.79 | ppl   120.89\n",
      "| epoch  17 |  3600/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.76 | ppl   117.22\n",
      "| epoch  17 |  3800/ 4833 batches | lr 2.09 | ms/batch 25.86 | loss  4.77 | ppl   117.96\n",
      "| epoch  17 |  4000/ 4833 batches | lr 2.09 | ms/batch 25.88 | loss  4.79 | ppl   119.88\n",
      "| epoch  17 |  4200/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.77 | ppl   117.41\n",
      "| epoch  17 |  4400/ 4833 batches | lr 2.09 | ms/batch 25.84 | loss  4.78 | ppl   118.59\n",
      "| epoch  17 |  4600/ 4833 batches | lr 2.09 | ms/batch 25.85 | loss  4.78 | ppl   118.55\n",
      "| epoch  17 |  4800/ 4833 batches | lr 2.09 | ms/batch 25.85 | loss  4.80 | ppl   121.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 165.11s | valid loss  5.99 | valid ppl   401.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 4833 batches | lr 1.99 | ms/batch 25.91 | loss  4.82 | ppl   124.58\n",
      "| epoch  18 |   400/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.77 | ppl   117.53\n",
      "| epoch  18 |   600/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.77 | ppl   118.49\n",
      "| epoch  18 |   800/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.80 | ppl   121.57\n",
      "| epoch  18 |  1000/ 4833 batches | lr 1.99 | ms/batch 25.86 | loss  4.81 | ppl   122.24\n",
      "| epoch  18 |  1200/ 4833 batches | lr 1.99 | ms/batch 25.89 | loss  4.76 | ppl   116.99\n",
      "| epoch  18 |  1400/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.77 | ppl   118.16\n",
      "| epoch  18 |  1600/ 4833 batches | lr 1.99 | ms/batch 25.85 | loss  4.79 | ppl   120.37\n",
      "| epoch  18 |  1800/ 4833 batches | lr 1.99 | ms/batch 25.82 | loss  4.79 | ppl   120.04\n",
      "| epoch  18 |  2000/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.76 | ppl   117.21\n",
      "| epoch  18 |  2200/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.76 | ppl   116.51\n",
      "| epoch  18 |  2400/ 4833 batches | lr 1.99 | ms/batch 25.90 | loss  4.74 | ppl   114.83\n",
      "| epoch  18 |  2600/ 4833 batches | lr 1.99 | ms/batch 25.84 | loss  4.73 | ppl   112.86\n",
      "| epoch  18 |  2800/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.72 | ppl   112.12\n",
      "| epoch  18 |  3000/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.76 | ppl   116.95\n",
      "| epoch  18 |  3200/ 4833 batches | lr 1.99 | ms/batch 25.93 | loss  4.76 | ppl   117.17\n",
      "| epoch  18 |  3400/ 4833 batches | lr 1.99 | ms/batch 25.92 | loss  4.77 | ppl   118.22\n",
      "| epoch  18 |  3600/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.74 | ppl   114.85\n",
      "| epoch  18 |  3800/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.75 | ppl   115.52\n",
      "| epoch  18 |  4000/ 4833 batches | lr 1.99 | ms/batch 25.86 | loss  4.76 | ppl   117.12\n",
      "| epoch  18 |  4200/ 4833 batches | lr 1.99 | ms/batch 25.86 | loss  4.75 | ppl   115.07\n",
      "| epoch  18 |  4400/ 4833 batches | lr 1.99 | ms/batch 25.86 | loss  4.75 | ppl   116.07\n",
      "| epoch  18 |  4600/ 4833 batches | lr 1.99 | ms/batch 25.88 | loss  4.76 | ppl   116.67\n",
      "| epoch  18 |  4800/ 4833 batches | lr 1.99 | ms/batch 25.87 | loss  4.78 | ppl   119.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 164.98s | valid loss  6.01 | valid ppl   405.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 4833 batches | lr 1.89 | ms/batch 26.01 | loss  4.81 | ppl   122.17\n",
      "| epoch  19 |   400/ 4833 batches | lr 1.89 | ms/batch 25.87 | loss  4.75 | ppl   115.29\n",
      "| epoch  19 |   600/ 4833 batches | lr 1.89 | ms/batch 25.92 | loss  4.75 | ppl   116.04\n",
      "| epoch  19 |   800/ 4833 batches | lr 1.89 | ms/batch 25.94 | loss  4.78 | ppl   119.19\n",
      "| epoch  19 |  1000/ 4833 batches | lr 1.89 | ms/batch 25.84 | loss  4.79 | ppl   120.15\n",
      "| epoch  19 |  1200/ 4833 batches | lr 1.89 | ms/batch 25.87 | loss  4.74 | ppl   115.00\n",
      "| epoch  19 |  1400/ 4833 batches | lr 1.89 | ms/batch 25.92 | loss  4.75 | ppl   115.68\n",
      "| epoch  19 |  1600/ 4833 batches | lr 1.89 | ms/batch 25.89 | loss  4.77 | ppl   118.10\n",
      "| epoch  19 |  1800/ 4833 batches | lr 1.89 | ms/batch 25.91 | loss  4.77 | ppl   117.88\n",
      "| epoch  19 |  2000/ 4833 batches | lr 1.89 | ms/batch 25.87 | loss  4.74 | ppl   114.92\n",
      "| epoch  19 |  2200/ 4833 batches | lr 1.89 | ms/batch 25.88 | loss  4.74 | ppl   114.24\n",
      "| epoch  19 |  2400/ 4833 batches | lr 1.89 | ms/batch 25.87 | loss  4.72 | ppl   112.71\n",
      "| epoch  19 |  2600/ 4833 batches | lr 1.89 | ms/batch 25.88 | loss  4.70 | ppl   110.37\n",
      "| epoch  19 |  2800/ 4833 batches | lr 1.89 | ms/batch 25.85 | loss  4.70 | ppl   110.22\n",
      "| epoch  19 |  3000/ 4833 batches | lr 1.89 | ms/batch 25.87 | loss  4.74 | ppl   114.42\n",
      "| epoch  19 |  3200/ 4833 batches | lr 1.89 | ms/batch 25.77 | loss  4.74 | ppl   114.89\n",
      "| epoch  19 |  3400/ 4833 batches | lr 1.89 | ms/batch 25.86 | loss  4.75 | ppl   115.71\n",
      "| epoch  19 |  3600/ 4833 batches | lr 1.89 | ms/batch 25.85 | loss  4.73 | ppl   112.82\n",
      "| epoch  19 |  3800/ 4833 batches | lr 1.89 | ms/batch 25.85 | loss  4.73 | ppl   113.13\n",
      "| epoch  19 |  4000/ 4833 batches | lr 1.89 | ms/batch 25.91 | loss  4.75 | ppl   115.15\n",
      "| epoch  19 |  4200/ 4833 batches | lr 1.89 | ms/batch 25.85 | loss  4.73 | ppl   113.16\n",
      "| epoch  19 |  4400/ 4833 batches | lr 1.89 | ms/batch 25.85 | loss  4.74 | ppl   114.15\n",
      "| epoch  19 |  4600/ 4833 batches | lr 1.89 | ms/batch 25.84 | loss  4.74 | ppl   114.14\n",
      "| epoch  19 |  4800/ 4833 batches | lr 1.89 | ms/batch 25.84 | loss  4.76 | ppl   116.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 165.03s | valid loss  6.01 | valid ppl   409.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 4833 batches | lr 1.79 | ms/batch 26.08 | loss  4.79 | ppl   119.84\n",
      "| epoch  20 |   400/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.73 | ppl   113.41\n",
      "| epoch  20 |   600/ 4833 batches | lr 1.79 | ms/batch 25.89 | loss  4.73 | ppl   113.75\n",
      "| epoch  20 |   800/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.76 | ppl   116.47\n",
      "| epoch  20 |  1000/ 4833 batches | lr 1.79 | ms/batch 25.87 | loss  4.77 | ppl   117.55\n",
      "| epoch  20 |  1200/ 4833 batches | lr 1.79 | ms/batch 25.91 | loss  4.73 | ppl   112.83\n",
      "| epoch  20 |  1400/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.73 | ppl   113.47\n",
      "| epoch  20 |  1600/ 4833 batches | lr 1.79 | ms/batch 25.87 | loss  4.75 | ppl   116.14\n",
      "| epoch  20 |  1800/ 4833 batches | lr 1.79 | ms/batch 25.87 | loss  4.75 | ppl   115.40\n",
      "| epoch  20 |  2000/ 4833 batches | lr 1.79 | ms/batch 25.89 | loss  4.72 | ppl   112.72\n",
      "| epoch  20 |  2200/ 4833 batches | lr 1.79 | ms/batch 25.90 | loss  4.72 | ppl   112.24\n",
      "| epoch  20 |  2400/ 4833 batches | lr 1.79 | ms/batch 25.87 | loss  4.71 | ppl   110.52\n",
      "| epoch  20 |  2600/ 4833 batches | lr 1.79 | ms/batch 25.84 | loss  4.69 | ppl   108.31\n",
      "| epoch  20 |  2800/ 4833 batches | lr 1.79 | ms/batch 25.85 | loss  4.68 | ppl   108.15\n",
      "| epoch  20 |  3000/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.72 | ppl   112.11\n",
      "| epoch  20 |  3200/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.72 | ppl   112.68\n",
      "| epoch  20 |  3400/ 4833 batches | lr 1.79 | ms/batch 25.89 | loss  4.73 | ppl   113.40\n",
      "| epoch  20 |  3600/ 4833 batches | lr 1.79 | ms/batch 25.87 | loss  4.70 | ppl   110.48\n",
      "| epoch  20 |  3800/ 4833 batches | lr 1.79 | ms/batch 25.85 | loss  4.71 | ppl   111.25\n",
      "| epoch  20 |  4000/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.73 | ppl   113.09\n",
      "| epoch  20 |  4200/ 4833 batches | lr 1.79 | ms/batch 25.86 | loss  4.71 | ppl   110.80\n",
      "| epoch  20 |  4400/ 4833 batches | lr 1.79 | ms/batch 25.91 | loss  4.72 | ppl   111.90\n",
      "| epoch  20 |  4600/ 4833 batches | lr 1.79 | ms/batch 25.90 | loss  4.72 | ppl   111.76\n",
      "| epoch  20 |  4800/ 4833 batches | lr 1.79 | ms/batch 25.85 | loss  4.74 | ppl   114.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 165.01s | valid loss  6.02 | valid ppl   410.92\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bptt = 35 #96\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_processed.cuda(), model.cuda(), criterion, optimizer, bptt, device)\n",
    "    val_loss = evaluate(model, test_processed, bptt)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_num = torch.tensor([vocab[tok] for tok in tokens]).cuda()\n",
    "    \n",
    "    src_mask = model.generate_square_subsequent_mask(len(tokens)).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_num, src_mask)\n",
    "    preds = outputs.view(-1, ntoken).argmax(1).tolist()\n",
    "    \n",
    "    return text + \" \" + \" \".join([vocab.itos[tok_idx] for tok_idx in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = train_df[0][3]#train_df.text[2][:100]#\"The idea of having Martin Scorsese in this movie\"\n",
    "# [vocab.itos[item] for item in process_sample(sample_text).argmax(dim=1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The idea of having a good movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The idea of having a good movie first , the to little . . first , the seen little . . movie . the to movie . . film . the a little . . movie . the to movie . . movie . a been little . . movie . the to movie . .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sample(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
