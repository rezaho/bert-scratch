{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"anchor\" id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Imports](#imports)\n",
    "* [2. Utils](#utils)\n",
    "    * [2.1. GeLu](#gelu)\n",
    "    * [2.2. Layer Norm](#layer_norm)\n",
    "    * [2.3. Feed Forward](#feedforward)\n",
    "* [3. Attention](#attention)\n",
    "    * [3.1. Single Attention](#single_attention)\n",
    "    * [3.2. Multi-head Attention](#multihead_attention)\n",
    "* [4. Encoder Layer](#encoder)\n",
    "    * [4.1. Sublayer Connection](#sublayer)\n",
    "    * [4.2. Position-wise Feedforward](#position_feedforward)\n",
    "    * [4.3. Transformer Block](#transformer_block)\n",
    "* [5. Embeddings](#embeddings)\n",
    "    * [5.1. Token Embedding](#token_embedding)\n",
    "    * [5.2. Position Embedding](#position_embedding)\n",
    "    * [5.3. Segment Embedding](#segment_embedding)\n",
    "    * [5.3. BERT Embedding](#bert_embedding)\n",
    "* [6. Model](#model)\n",
    "    * [6.1. BERT](#bert)\n",
    "    * [6.2. Masked Language Model](#masked_lm)\n",
    "    * [6.3. Next Sentence Prediction](#nsp)\n",
    "    * [6.4. BERT Language Model](#bert_lm)\n",
    "\n",
    "* [7. Dataset](#dataset)\n",
    "    * [7.1. Vocabulary Building](#vocab)\n",
    "    * [7.2. Dataset Loading](#dataset_load)\n",
    "* [8. Trainer](#trainer)\n",
    "    * [8.1. Optimizer Scheduler](#optim_schedule)\n",
    "    * [8.2. Pre-train](#pretrain)\n",
    "* [9. Fine-tuning](#finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports <a class=\"anchor\" id=\"imports\"></a> \n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Utils <a class=\"anchor\" id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. GeLu <a class=\"anchor\" id=\"gelu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Layer Norm <a class=\"anchor\" id=\"layer_norm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" \n",
    "    Construct a layer norm \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Feed Forward <a class=\"anchor\" id=\"feedforward\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Attention <a class=\"anchor\" id=\"attention\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Single attention <a class=\"anchor\" id=\"single_attention\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Computes 'Scaled Dot Product Attention' \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "            \n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Multi-head attention <a class=\"anchor\" id=\"multihead_attention\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .single import Attention\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-headed Attention - Takes in `model size` and `number of heads`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        # Assuming always d_k = d_v\"\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        # 1- Do all projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linear_layers, (query, key, value))\n",
    "        ]\n",
    "        # 2- Apply attention on all of the projected vectors in batch (`h` attention head)\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 3- \"Concat\" all attention heads using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoder Layer <a class=\"anchor\" id=\"encoder\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Sublayer Connection <a class=\"anchor\" id=\"sublayer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .layer_nor import LayerNorm\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a residual network followed by a layer norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Position-wise Feedforward <a class=\"anchor\" id=\"position_feedforward\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .gelu import GELU\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a feed-forward network layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        self.activation = GELU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(\n",
    "            self.dropout(\n",
    "                self.activation(self.w_1(x))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Transformer Block <a class=\"anchor\" id=\"transformer_block\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .utils import SublayerConnection, PositionwiseFeedForward\n",
    "# from .attention import MultiHeadedAttention\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer(self-attention)\n",
    "    \n",
    "    Transformer = Multiheaded_Attention + FeedForward with Sublayer conneciton\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: size of hidden layer of transformer (or `d_model`)\n",
    "        :param attn_heads: number of attention heads in multi-headed attention layer\n",
    "        :param feed_forward_hidden: size of feed forward hidden layer (usually 4*hidden)\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Embeddings <a class=\"anchor\" id=\"embeddings\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Token Embedding <a class=\"anchor\" id=\"token_embedding\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Position Embedding <a class=\"anchor\" id=\"position_embedding\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # compute the positional encoding\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        \n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        division = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * division)\n",
    "        pe[:, 1::2] = torch.cos(position * division)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        # Register buffer to add persistent state without counting it as a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Segment Embedding <a class=\"anchor\" id=\"segment_embedding\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0) # vocab size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. BERT Embedding <a class=\"anchor\" id=\"bert_embedding\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from .token import TokenEmbedding\n",
    "# from .position import PositionalEmbedding\n",
    "# from .segment import SegmentEmbedding\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT embedding is consisted of three different embeddings:\n",
    "        - Token Embedding\n",
    "        - Positional Embedding\n",
    "        - Segment Embedding\n",
    "    The final embedding is the summation of all these three embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, dropout):\n",
    "        \"\"\"\n",
    "        :param vocab_size\t: size of vocabulary\n",
    "        :param embed_size\t: size of token embedding\n",
    "        :param dropout\t\t: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model <a class=\"anchor\" id=\"model\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. BERT <a class=\"anchor\" id=\"bert\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# from .transformer import Transformer\n",
    "# from .embedding import BERTEmbedding\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model: Bidirectional Encoder Representations from Transformers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param hidden: size of BERT hidden layers\n",
    "        :param n_layers: number of Transformer blocks (layers)\n",
    "        :param attn_heads: number of attention heads in multi-headed attention\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden, dropout=dropout)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            Transformer(\n",
    "                hidden=hidden, \n",
    "                attn_heads=attn_heads, \n",
    "                feed_forward_hidden=self.feed_forward_hidden, \n",
    "                dropout=dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, segment_labels):\n",
    "        # Creating attention mask for padding tokens\n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        x = self.embedding(x, segment_labels)\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            x = transformer.forward(x, mask=mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Masked Language Model <a class=\"anchor\" id=\"masked_lm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    n-class classification module (n-class = vocab_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden : size of hidden layers in Transformer block\n",
    "        :param vocab_size : size of vocabulary (= n_class)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Next Sentence Prediction <a class=\"anchor\" id=\"nsp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model: `is_next`, `is_not_next`\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT hidden layer size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.4. BERT Language Model <a class=\"anchor\" id=\"bert_lm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model (with NSP + MLM)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert : BERT model to be trained\n",
    "        :param vocab_size : vocabulary size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.masked_lm = MaskedLanguageModel(hidden=self.bert.hidden, vocab_size=vocab_size)\n",
    "        \n",
    "    def forward(self, x, segment_labels):\n",
    "        x = self.bert(x, segment_labels)\n",
    "        return self.next_sentence(x), self.masked_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Vocabulary Building <a class=\"anchor\" id=\"vocab\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "# from multiprocessing import Pool\n",
    "from pathos.multiprocessing import ProcessPool as Pool\n",
    "\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(\n",
    "        self, counter, max_size=None, min_freq=1, \n",
    "        pad_token=\"[PAD]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    ):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.sep_index = 2\n",
    "        self.cls_index = 3\n",
    "        self.mask_index = 4\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.sep_token = sep_token\n",
    "        self.cls_token = cls_token\n",
    "        self.mask_token = mask_token\n",
    "        \n",
    "        super().__init__(counter, specials=[pad_token, unk_token, sep_token, cls_token, mask_token],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_sep=False, with_cls=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(\n",
    "        self, texts, tok_train_path, max_size=None, min_freq=1,\n",
    "        pad_token=\"[PAD]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param texts : List of sentences for building the vocabulary\n",
    "        :param tok_train_path : Path to training texts for training the tokenizer\n",
    "        :param pad_token : Special token to be used for padding\n",
    "        :param unk_token : Special token to be used for unknown words\n",
    "        :param sep_token : Special token to be used as seperator\n",
    "        :param cls_token : Special token to be used as CLS\n",
    "        :param mask_token : Special token to be used for attention masking\n",
    "        \"\"\"\n",
    "        \n",
    "        # Creating Tokenizer for building the vocabulary\n",
    "        self.tokenizer = BertWordPieceTokenizer()\n",
    "        print(\"Training the tokenizer to build the vocabulary...\")\n",
    "        self.tokenizer.train(tok_train_path)\n",
    "        print(\"Training tokenizer finished successfully!\\n\")\n",
    "        \n",
    "        # Building the vocabulary\n",
    "        print(\"Building Vocab...\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm(texts, total=len(texts)):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = self.tokenizer.encode(line).tokens\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "                \n",
    "        super().__init__(\n",
    "            counter, max_size=max_size, min_freq=min_freq, \n",
    "            pad_token=pad_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token\n",
    "        )\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_sep=False, with_cls=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = self.tokenizer.encode(sentence).tokens\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_sep:\n",
    "            seq += [self.sep_index]  # this would be index 1\n",
    "        if with_cls:\n",
    "            seq = [self.cls_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if with_pad or idx != self.pad_index]\n",
    "\n",
    "        return (\" \".join(words)).replace(\" ##\", \"\") if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "#     def parallel_processing(self, texts, ncores=None, message=None):\n",
    "#         # from pathos.multiprocessing import ProcessPool as Pool\n",
    "#         p = Pool(ncores)\n",
    "#         return tqdm(\n",
    "#             p.imap(self.count_each_line, texts),\n",
    "#             total=len(texts), \n",
    "#             desc=message\n",
    "#         )\n",
    "    \n",
    "#     def count_each_line(self, line):\n",
    "#         if isinstance(line, list):\n",
    "#             words = line\n",
    "#         else:\n",
    "#             words = self.tokenizer.encode(line).tokens\n",
    "            \n",
    "#         return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary.from_seq(vocabulary.to_seq(train_df['text'][0], seq_len=155), join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_bert(sents):\n",
    "    print(\"Starting Batch Tokenization...\")\n",
    "    tokenized_sents = vocabulary.tokenizer.encode_batch(sents)\n",
    "    return [\n",
    "        \" \".join(x.tokens) \n",
    "        for x in tqdm(tokenized_sents, total=len(tokenized_sents), desc=\"Merging tokens...\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-335a7f33c60c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time elapsed {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "train_df['text'] = pd.Series(tokenize_for_bert(train_df.text))\n",
    "print(\"Time elapsed {:.2f}\".format(time.perf_counter()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./wikitext-103/wikitext103_bert_train_tokenized.csv\", sep=\"\\t\", index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Dataset <a class=\"anchor\" id=\"dataset_load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "            if self.corpus_lines is None and not on_memory:\n",
    "                for _ in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                    self.corpus_lines += 1\n",
    "\n",
    "            if on_memory:\n",
    "                self.lines = [line[:-1].split(\"\\t\")\n",
    "                              for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)]\n",
    "                self.corpus_lines = len(self.lines)\n",
    "\n",
    "        if not on_memory:\n",
    "            self.file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            self.random_file = open(corpus_path, \"r\", encoding=encoding)\n",
    "\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        t1 = [self.vocab.cls_index] + t1_random + [self.vocab.sep_index]\n",
    "        t2 = t2_random + [self.vocab.sep_index]\n",
    "\n",
    "        t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n",
    "        t2_label = t2_label + [self.vocab.pad_index]\n",
    "\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "\n",
    "        padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.randrange(len(self.vocab))\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "\n",
    "                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "            else:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                output_label.append(0)\n",
    "\n",
    "        return tokens, output_label\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # output_text, label(isNotNext:0, isNext:1)\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        if self.on_memory:\n",
    "            if self.lines[item][1]==0 or item==0:\n",
    "                return self.lines[item][0], self.lines[item+1][0]\n",
    "            else:\n",
    "                return self.lines[item-1][0], self.lines[item][0]\n",
    "        else:\n",
    "            line_1 = self.file.__next__()\n",
    "            line_2 = self.file.__next__()\n",
    "            if line_1 is None:\n",
    "                self.file.close()\n",
    "                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "                line_1 = self.file.__next__()\n",
    "                line_2 = self.file.__next__()\n",
    "            elif line_2 is None:\n",
    "                self.file.close()\n",
    "                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "                line_1 = self.file.__next__()\n",
    "                line_2 = self.file.__next__()\n",
    "            line_1 = line_1[:-1].split(\"\\t\")\n",
    "            line_2 = line_2[:-1].split(\"\\t\")\n",
    "            if line_2[1]==1:\n",
    "                return line_1[0], line_2[0]\n",
    "            \n",
    "            line_3 = self.file.__next__()\n",
    "            if line_3 is None:\n",
    "                self.file.close()\n",
    "                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "                line_1 = self.file.__next__()\n",
    "                line_2 = self.file.__next__()\n",
    "                line_1 = line_1[:-1].split(\"\\t\")\n",
    "                line_2 = line_2[:-1].split(\"\\t\")\n",
    "                return line_1[0], line_2[0]\n",
    "            return line_2[0], line_3[:-1].split(\"\\t\")[0]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        if self.on_memory:\n",
    "            rand_idx = random.randrange(len(self.lines))\n",
    "            return self.lines[rand_idx][0]\n",
    "\n",
    "        line = self.file.__next__()\n",
    "        if line is None:\n",
    "            self.file.close()\n",
    "            self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "            line = self.random_file.__next__()\n",
    "        return line[:-1].split(\"\\t\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class BERTDatasetOnMemory(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True, device='cuda'):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "            self.lines = [\n",
    "                line[:-1].split(\"\\t\")\n",
    "                for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)\n",
    "            ]\n",
    "            self.corpus_len = len(self.lines)\n",
    "        self.sentences, self.isnext = self._preprocess_lines(self.lines)\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available() and device=='cuda':\n",
    "            self.device = 'cuda'\n",
    "            self.sentences = [t.cuda() for t in tqdm(self.sentences, desc=\"Moving sentences to GPU...\")]\n",
    "            self.isnext = self.isnext.cuda()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.corpus_len\n",
    "    \n",
    "    \n",
    "    def to(self, device):\n",
    "        self.sentences = [x.to(device) for x in tqdm(self.sentences, desc=f\"Moving sentences to {device.upper()}...\")]\n",
    "        self.isnext = self.isnext.to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def _preprocess_lines(self, lines):\n",
    "        train_sents = []\n",
    "        train_isnext = []\n",
    "        for line in tqdm(lines):\n",
    "            tokens = line[0].split()\n",
    "            tokens_id = [self.vocab.stoi.get(tok, self.vocab.unk_index) for tok in tokens]\n",
    "            train_sents.append(torch.tensor(tokens_id).long())\n",
    "            train_isnext.append(int(line[1]))\n",
    "        train_isnext = torch.tensor(train_isnext).long()\n",
    "        \n",
    "        return train_sents, train_isnext\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        with torch.no_grad():\n",
    "            t1 = torch.cat([torch.tensor([self.vocab.cls_index]).to(self.device), t1_random, torch.tensor([self.vocab.sep_index]).to(self.device)])\n",
    "            t2 =  torch.cat([t2_random, torch.tensor([self.vocab.sep_index]).to(self.device)])\n",
    "\n",
    "            t1_label = torch.cat([torch.tensor([self.vocab.pad_index]).to(self.device), t1_label, torch.tensor([self.vocab.pad_index]).to(self.device)])\n",
    "            t2_label = torch.cat([t2_label, torch.tensor([self.vocab.pad_index]).to(self.device)])\n",
    "\n",
    "            segment_label = torch.tensor(([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]).to(self.device)\n",
    "            bert_input = torch.cat([t1, t2])[:self.seq_len]\n",
    "            bert_label = torch.cat([t1_label, t2_label])[:self.seq_len]\n",
    "\n",
    "            padding = torch.tensor([self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]).to(self.device)\n",
    "            bert_input = torch.cat([bert_input, padding]).detach()\n",
    "            bert_label = torch.cat([bert_label, padding]).detach()\n",
    "            segment_label = torch.cat([segment_label, padding]).detach()\n",
    "\n",
    "        return {\n",
    "            \"bert_input\": bert_input.long(),\n",
    "            \"bert_label\": bert_label,\n",
    "            \"segment_label\": segment_label,\n",
    "            \"is_next\": is_next_label\n",
    "        }\n",
    "#         return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    \n",
    "    def random_word(self, tokens):\n",
    "        tokens = tokens.detach().clone()\n",
    "        output_label = []\n",
    "        with torch.no_grad():\n",
    "            for i, token in enumerate(tokens):\n",
    "                prob = random.random()\n",
    "                if prob < 0.15:\n",
    "                    prob /= 0.15\n",
    "\n",
    "                    # 80% randomly change token to mask token\n",
    "                    if prob < 0.8:\n",
    "                        tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                    # 10% randomly change token to random token\n",
    "                    elif prob < 0.9:\n",
    "                        tokens[i] = int(random.randrange(len(self.vocab)))\n",
    "\n",
    "                    # 10% randomly change token to current token\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    output_label.append(token)\n",
    "\n",
    "                else:\n",
    "                    # tokens[i] = token\n",
    "                    output_label.append(0)\n",
    "\n",
    "        return tokens, torch.tensor(output_label).to(self.device)\n",
    "\n",
    "    \n",
    "    def random_sent(self, index):\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # output_text, label(isNotNext:0, isNext:1)\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "        \n",
    "    def get_corpus_line(self, item):\n",
    "        if self.isnext[item]==0 or item==0:\n",
    "            return self.sentences[item], self.sentences[item+1]\n",
    "        else:\n",
    "            return self.sentences[item-1], self.sentences[item]\n",
    "\n",
    "\n",
    "    def get_random_line(self):\n",
    "        rand_idx = random.randrange(len(self.lines))\n",
    "        return self.sentences[rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenization\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Wordpiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, unk_token, max_input_chars_per_word=100):\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "\n",
    "        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer():\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=vocab, unk_token=self.vocab.unk_token)\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        return self.wordpiece_tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab.stoi.get(token, self.vocab.unk_index) for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        tokens = [self.vocab.itos[id] for id in token_ids]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def _batch_encoding(self, ):\n",
    "        pass # TO-DO\n",
    "    \n",
    "    def _pad(self, encoding):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Trainer <a class=\"anchor\" id=\"trainer\"></a>\n",
    "-> *[Top](#top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Optimizer Scheduler <a class=\"anchor\" id=\"optim_schedule\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''A wrapper class for optimizer '''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Pre-train <a class=\"anchor\" id=\"pretrain\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from typing import Union\n",
    "# from ..model import BERTLM, BERT\n",
    "# from .optim_schedule import ScheduledOptim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "    please check the details on README.md with simple example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                 with_cuda: bool = True, cuda_devices=None, log_freq: int = 10, \n",
    "                 checkpoint_path: Union[str, None] = None, bert_model_path: Union[str, None] = None):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # Either initializing BERT or loading from the last checkpoint\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        if bert_model_path is not None:\n",
    "            print(\"Loading from checkpoint...\")\n",
    "            self.load_bert(path=bert_model_path, type=\"entire\")\n",
    "            \n",
    "        # Initialize the BERT Language Model or load it from checkpoint_path, with BERT model\n",
    "        self.model = BERTLM(bert, vocab_size)\n",
    "        if checkpoint_path is not None:\n",
    "            self.load_from_checkpoint(path=checkpoint_path)\n",
    "        # Sending the model to the appropriate device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        if with_cuda and torch.cuda.device_count() > 1:\n",
    "            print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            str_code = \"train\"\n",
    "            self.model.train()\n",
    "        else:\n",
    "            str_code = \"test\"\n",
    "            self.model.eval()\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm(enumerate(data_loader),\n",
    "                         desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                         total=len(data_loader),\n",
    "                         bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "            if i % self.log_freq == 0:\n",
    "                logging.info(json.dumps(post_fix))\n",
    "            \n",
    "            if i % (2*self.log_freq) == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path=\"output/bert_trained.model\"):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = file_path + \".{}.ep{:02d}.pt\" #% epoch\n",
    "        # Saving Entire BERT model\n",
    "        torch.save(self.bert.cpu(), output_path.format(\"bert.model\", epoch))\n",
    "        # Saving only BERT state_dict\n",
    "        torch.save(self.bert.cpu().state_dict(), output_path.format(\"bert.statedict\", epoch))\n",
    "        # Saving BERT LM state_dict\n",
    "        torch.save(self.model.cpu().state_dict(), output_path.format(\"bertlm.statedict\", epoch))\n",
    "        \n",
    "        self.bert.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "    \n",
    "    def load_bert(self, path, type=\"entire\"):\n",
    "        if type==\"entire\":\n",
    "            # Loading entire model\n",
    "            self.bert = torch.load(path)\n",
    "        elif type==\"state_dict\":\n",
    "            self.bert.load_state_dict(torch.load(path))\n",
    "        else:\n",
    "            raise TypeError('Parameter `type` can either be \"entire\" or \"state_dict\"')\n",
    "            \n",
    "    def load_from_checkpoint(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Preparing Datasets <a class=\"anchor\" id=\"finetuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Preparing Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794,232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senj no Valkyria 3 : [UNK] Chronicles ( Japan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The game began development in 2010 , carrying ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_next\n",
       "0  Senj no Valkyria 3 : [UNK] Chronicles ( Japan...        0\n",
       "1  The game began development in 2010 , carrying ...        1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"./wikitext-103/wikitext103_bert_train.csv\", \n",
    "    sep=\"\\t\", \n",
    "    header=None, \n",
    "    names=['text', 'is_next']\n",
    ")\n",
    "print(\"{:,}\".format(train_df.shape[0]))\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = WordVocab(train_df['text'], \"./wikitext-103/wikitext103_bert_train_for_tokenizer.txt\")\n",
    "# vocabulary.save_vocab(\"./wikitext-103/vocabulary.pkl\")\n",
    "\n",
    "# vocabulary = WordVocab.load_vocab(\"./wikitext-103/vocabulary.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Preparing Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Pre-training <a class=\"anchor\" id=\"finetuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='./models/logging/bert_01_logs.jsonl', \n",
    "    filemode='a', \n",
    "    encoding='utf-8', \n",
    "    level=logging.INFO,\n",
    "    format='{\"name\": \"%(name)s\", \"levelname\": \"%(levelname)s\", \"message\":%(message)s}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"./models/logging/bert_01_logs.jsonl\", \"r\") as file:\n",
    "#     json_list = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_list = [\n",
    "#     {'epoch': 0, 'iter': 0, 'avg_loss': 13.269269943237305, 'avg_acc': 50.0, 'loss': 13.269269943237305},\n",
    "#     {'epoch': 0, 'iter': 2000, 'avg_loss': 6.469618820417291, 'avg_acc': 49.346160253206726, 'loss': 6.091062068939209},\n",
    "#     {'epoch': 0, 'iter': 4000, 'avg_loss': 6.041361987426918, 'avg_acc': 49.83962342747647, 'loss': 5.788421630859375},\n",
    "#     {'epoch': 0, 'iter': 6000, 'avg_loss': 5.8731231118138485, 'avg_acc': 49.78198078098095, 'loss': 6.124691486358643},\n",
    "#     {'epoch': 0, 'iter': 8000, 'avg_loss': 5.796649547803851, 'avg_acc': 49.91876015498063, 'loss': 5.252473831176758},\n",
    "#     {'epoch': 0, 'iter': 10000, 'avg_loss': 5.757141550664079, 'avg_acc': 49.99250074992501, 'loss': 5.519923686981201},\n",
    "#     {'epoch': 0, 'iter': 12000, 'avg_loss': 5.7377515047771235, 'avg_acc': 49.97500208315974, 'loss': 6.160713195800781},\n",
    "#     {'epoch': 0, 'iter': 14000, 'avg_loss': 5.727109319763042, 'avg_acc': 50.09166011951527, 'loss': 5.713372707366943},\n",
    "#     {'epoch': 0, 'iter': 16000, 'avg_loss': 5.719262601226488, 'avg_acc': 50.09166093785805, 'loss': 5.480091571807861},\n",
    "#     {'epoch': 0, 'iter': 18000, 'avg_loss': 5.712273025504748, 'avg_acc': 50.11480843656834, 'loss': 5.6654486656188965},\n",
    "#     {'epoch': 0, 'iter': 20000, 'avg_loss': 5.705449273077156, 'avg_acc': 50.10624468776561, 'loss': 5.460890769958496},\n",
    "#     {'epoch': 0, 'iter': 22000, 'avg_loss': 5.699919344560596, 'avg_acc': 50.06779994848719, 'loss': 5.916445732116699},\n",
    "#     {'epoch': 0, 'iter': 24000, 'avg_loss': 5.694113352999956, 'avg_acc': 50.00694415510465, 'loss': 5.917047023773193},\n",
    "#     {'epoch': 0, 'iter': 26000, 'avg_loss': 5.68845057999151, 'avg_acc': 49.98076997038575, 'loss': 5.599635124206543},\n",
    "#     {'epoch': 0, 'iter': 28000, 'avg_loss': 5.682388940967281, 'avg_acc': 49.97559610966275, 'loss': 5.874852657318115},\n",
    "#     {'epoch': 0, 'iter': 30000, 'avg_loss': 5.679349764510515, 'avg_acc': 49.9733342221926, 'loss': 5.783846855163574},\n",
    "#     {'epoch': 0, 'iter': 32000, 'avg_loss': 5.676068597128829, 'avg_acc': 49.969532202118685, 'loss': 5.7647223472595215},\n",
    "#     {'epoch': 0, 'iter': 34000, 'avg_loss': 5.67338184042435, 'avg_acc': 49.974755644441835, 'loss': 5.24131441116333},\n",
    "#     {'epoch': 0, 'iter': 36000, 'avg_loss': 5.670176511234563, 'avg_acc': 49.965741692360396, 'loss': 6.289365291595459},\n",
    "#     {'epoch': 0, 'iter': 38000, 'avg_loss': 5.6673862358723275, 'avg_acc': 49.952413532977204, 'loss': 6.110634803771973},\n",
    "#     {'epoch': 0, 'iter': 40000, 'avg_loss': 5.664838870249195, 'avg_acc': 49.97625059373515, 'loss': 5.284783840179443},\n",
    "#     {'epoch': 0, 'iter': 42000, 'avg_loss': 5.6633376914834255, 'avg_acc': 49.98115124243074, 'loss': 5.691641807556152},\n",
    "#     {'epoch': 0, 'iter': 44000, 'avg_loss': 5.6601254738248725, 'avg_acc': 49.978220191965335, 'loss': 5.555965423583984},\n",
    "#     {'epoch': 0, 'iter': 46000, 'avg_loss': 5.658735998568153, 'avg_acc': 49.99257262523278, 'loss': 5.5244574546813965},\n",
    "#     {'epoch': 0, 'iter': 48000, 'avg_loss': 5.657340729247282, 'avg_acc': 49.987153045422666, 'loss': 5.798534393310547},\n",
    "#     {'epoch': 0, 'iter': 50000, 'avg_loss': 5.655262788694689, 'avg_acc': 49.986666933328, 'loss': 5.248673915863037},\n",
    "#     {'epoch': 0, 'iter': 52000, 'avg_loss': 5.652675138469182, 'avg_acc': 49.99310910687615, 'loss': 5.325676918029785},\n",
    "#     {'epoch': 0, 'iter': 54000, 'avg_loss': 5.6506219999372655, 'avg_acc': 49.988889094646396, 'loss': 5.404134273529053},\n",
    "#     {'epoch': 0, 'iter': 56000, 'avg_loss': 5.648724899477444, 'avg_acc': 50.0047618197294, 'loss': 5.765843868255615},\n",
    "#     {'epoch': 0, 'iter': 58000, 'avg_loss': 5.64811705611097, 'avg_acc': 50.010344649230184, 'loss': 5.437739849090576},\n",
    "#     {'epoch': 0, 'iter': 60000, 'avg_loss': 5.64712778384029, 'avg_acc': 49.99861113425887, 'loss': 5.900131702423096},\n",
    "#     {'epoch': 0, 'iter': 62000, 'avg_loss': 5.6453301490667345, 'avg_acc': 49.98669376300382, 'loss': 5.9393134117126465},\n",
    "#     {'epoch': 0, 'iter': 64000, 'avg_loss': 5.644005112799925, 'avg_acc': 49.992708447263844, 'loss': 5.701573371887207},\n",
    "#     {'epoch': 0, 'iter': 66000, 'avg_loss': 5.6424403046841345, 'avg_acc': 49.99444452861825, 'loss': 5.566465377807617}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in json_list:\n",
    "#     logging.info(json.dumps(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b71a58ac35453a8f958b63f930f214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading Dataset', max=794232.0, style=ProgressStyle(descr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_path = \"./wikitext-103/vocabulary.pkl\"\n",
    "train_ds_path = \"./wikitext-103/wikitext103_bert_train_tokenized.csv\"\n",
    "seq_len = 512\n",
    "corpus_lines = int(train_df.shape[0])\n",
    "on_memory = True\n",
    "batch_size = 12\n",
    "num_workers = 22\n",
    "attn_heads = 12\n",
    "layers = 12\n",
    "hidden = 768\n",
    "lr = 3e-4\n",
    "adam_beta1, adam_beta2 = 0.9, 0.999\n",
    "adam_weight_decay = 0.01\n",
    "with_cuda = True\n",
    "cuda_devices = 1\n",
    "log_freq = 1000\n",
    "epochs = 3\n",
    "output_path = \"./models/bert_01\"\n",
    "test_dataset = None\n",
    "vocab = WordVocab.load_vocab(vocab_path)\n",
    "train_dataset = BERTDataset(train_ds_path, vocab, seq_len=seq_len,\n",
    "                            corpus_lines=corpus_lines, on_memory=on_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataloader\n",
      "Building BERT model\n",
      "Creating BERT Trainer\n",
      "Total Parameters: 130049370\n"
     ]
    }
   ],
   "source": [
    "# print(\"Loading Test Dataset\", args.test_dataset)\n",
    "# test_dataset = BERTDataset(args.test_dataset, vocab, seq_len=args.seq_len, on_memory=args.on_memory) \\\n",
    "#     if args.test_dataset is not None else None\n",
    "\n",
    "print(\"Creating Dataloader\")\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=22)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=22) \\\n",
    "    if test_dataset is not None else None\n",
    "\n",
    "print(\"Building BERT model\")\n",
    "bert = BERT(len(vocab), hidden=hidden, n_layers=layers, attn_heads=attn_heads)\n",
    "\n",
    "print(\"Creating BERT Trainer\")\n",
    "trainer = BERTTrainer(bert, len(vocab), train_dataloader=train_data_loader, test_dataloader=test_data_loader,\n",
    "                      lr=lr, betas=(adam_beta1, adam_beta2), weight_decay=adam_weight_decay,\n",
    "                      with_cuda=with_cuda, cuda_devices=cuda_devices, log_freq=log_freq, \n",
    "                      checkpoint_path=\"./models/bert_01.bertlm.statedict.ep02.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b524e6103aa14702aa8c60877ceda457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EP_train:1', max=66186.0, style=ProgressStyle(description"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 13.376056671142578, 'avg_acc': 75.0, 'loss': 13.376056671142578}\n",
      "{'epoch': 1, 'iter': 2000, 'avg_loss': 6.487176100651304, 'avg_acc': 49.75012493753124, 'loss': 5.912215232849121}\n",
      "{'epoch': 1, 'iter': 4000, 'avg_loss': 6.051530485718109, 'avg_acc': 49.635507789719234, 'loss': 5.77377462387085}\n",
      "{'epoch': 1, 'iter': 6000, 'avg_loss': 5.877502279388092, 'avg_acc': 49.8666888851858, 'loss': 6.1501545906066895}\n",
      "{'epoch': 1, 'iter': 8000, 'avg_loss': 5.797374949904028, 'avg_acc': 49.94479856684581, 'loss': 5.3463664054870605}\n",
      "{'epoch': 1, 'iter': 10000, 'avg_loss': 5.75829712899491, 'avg_acc': 49.954171249541716, 'loss': 5.597848892211914}\n",
      "{'epoch': 1, 'iter': 12000, 'avg_loss': 5.739098143631216, 'avg_acc': 49.946532233424996, 'loss': 6.111310958862305}\n",
      "{'epoch': 1, 'iter': 14000, 'avg_loss': 5.728710279729688, 'avg_acc': 49.92500535676023, 'loss': 5.169239044189453}\n",
      "{'epoch': 1, 'iter': 16000, 'avg_loss': 5.721339396734281, 'avg_acc': 49.99635439451701, 'loss': 5.368520736694336}\n",
      "{'epoch': 1, 'iter': 18000, 'avg_loss': 5.714712435057094, 'avg_acc': 49.99259300409237, 'loss': 5.9694976806640625}\n",
      "{'epoch': 1, 'iter': 20000, 'avg_loss': 5.707654980777305, 'avg_acc': 49.935836541506255, 'loss': 5.751869201660156}\n",
      "{'epoch': 1, 'iter': 22000, 'avg_loss': 5.70224753706397, 'avg_acc': 49.95719891520082, 'loss': 6.095515727996826}\n",
      "{'epoch': 1, 'iter': 24000, 'avg_loss': 5.696334877489388, 'avg_acc': 49.91840617752038, 'loss': 5.816843032836914}\n",
      "{'epoch': 1, 'iter': 26000, 'avg_loss': 5.6909283501960415, 'avg_acc': 49.92916939092086, 'loss': 5.675166606903076}\n",
      "{'epoch': 1, 'iter': 28000, 'avg_loss': 5.685891537209765, 'avg_acc': 49.939883099413116, 'loss': 5.89703893661499}\n",
      "{'epoch': 1, 'iter': 30000, 'avg_loss': 5.68231504774305, 'avg_acc': 49.94222414808395, 'loss': 5.823694229125977}\n",
      "{'epoch': 1, 'iter': 32000, 'avg_loss': 5.679298430240727, 'avg_acc': 49.90521129548035, 'loss': 5.936532974243164}\n",
      "{'epoch': 1, 'iter': 34000, 'avg_loss': 5.67666934070922, 'avg_acc': 49.92059057086556, 'loss': 5.359133243560791}\n",
      "{'epoch': 1, 'iter': 36000, 'avg_loss': 5.673480039430835, 'avg_acc': 49.94652926307603, 'loss': 6.068933963775635}\n",
      "{'epoch': 1, 'iter': 38000, 'avg_loss': 5.67058136706534, 'avg_acc': 49.989912546161065, 'loss': 5.785726547241211}\n",
      "{'epoch': 1, 'iter': 40000, 'avg_loss': 5.667976747014869, 'avg_acc': 49.9889586093681, 'loss': 5.48654842376709}\n",
      "{'epoch': 1, 'iter': 42000, 'avg_loss': 5.666117560252988, 'avg_acc': 50.004364975437085, 'loss': 6.024386882781982}\n",
      "{'epoch': 1, 'iter': 44000, 'avg_loss': 5.6628964323988935, 'avg_acc': 50.010795209199784, 'loss': 5.879703521728516}\n",
      "{'epoch': 1, 'iter': 46000, 'avg_loss': 5.661069353363653, 'avg_acc': 49.98641333884046, 'loss': 5.497503280639648}\n",
      "{'epoch': 1, 'iter': 48000, 'avg_loss': 5.659905648669095, 'avg_acc': 50.01128448712874, 'loss': 5.87322473526001}\n",
      "{'epoch': 1, 'iter': 50000, 'avg_loss': 5.657921171277044, 'avg_acc': 50.001166643333804, 'loss': 5.398408889770508}\n",
      "{'epoch': 1, 'iter': 52000, 'avg_loss': 5.655321398239127, 'avg_acc': 49.98637846708076, 'loss': 5.504925727844238}\n",
      "{'epoch': 1, 'iter': 54000, 'avg_loss': 5.653098487643846, 'avg_acc': 49.99351863854373, 'loss': 5.250624179840088}\n",
      "{'epoch': 1, 'iter': 56000, 'avg_loss': 5.65109601271744, 'avg_acc': 50.0037201716636, 'loss': 5.932851791381836}\n",
      "{'epoch': 1, 'iter': 58000, 'avg_loss': 5.650451485044933, 'avg_acc': 49.99827589179497, 'loss': 5.6197710037231445}\n",
      "{'epoch': 1, 'iter': 60000, 'avg_loss': 5.649241192229868, 'avg_acc': 49.97666705554907, 'loss': 5.9310102462768555}\n",
      "{'epoch': 1, 'iter': 62000, 'avg_loss': 5.647334301180036, 'avg_acc': 49.97298430670473, 'loss': 5.405673503875732}\n",
      "{'epoch': 1, 'iter': 64000, 'avg_loss': 5.6458267244599245, 'avg_acc': 49.963151617422646, 'loss': 5.535201549530029}\n",
      "{'epoch': 1, 'iter': 66000, 'avg_loss': 5.644582410763308, 'avg_acc': 49.958839007489786, 'loss': 5.792216777801514}\n",
      "\n",
      "EP1_train, avg_loss= 5.644394274052311 total_acc= 49.953665931365144\n",
      "EP:1 Model Saved on: ./models/bert_01.{0}.ep{1:2d}.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b552365b1a4e08829331b9a0a9655d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='EP_train:2', max=66186.0, style=ProgressStyle(description"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 6.172450542449951, 'avg_acc': 66.66666666666666, 'loss': 6.172450542449951}\n",
      "{'epoch': 2, 'iter': 2000, 'avg_loss': 5.6135421574920015, 'avg_acc': 49.891720806263535, 'loss': 5.991109848022461}\n",
      "{'epoch': 2, 'iter': 4000, 'avg_loss': 5.612320837453734, 'avg_acc': 50.1374656335916, 'loss': 5.446863651275635}\n",
      "{'epoch': 2, 'iter': 6000, 'avg_loss': 5.612621113412123, 'avg_acc': 50.0888740765428, 'loss': 6.237857341766357}\n",
      "{'epoch': 2, 'iter': 8000, 'avg_loss': 5.611455500178033, 'avg_acc': 50.06457526142566, 'loss': 5.117506504058838}\n",
      "{'epoch': 2, 'iter': 10000, 'avg_loss': 5.608227212147026, 'avg_acc': 50.07582575075826, 'loss': 5.589437484741211}\n",
      "{'epoch': 2, 'iter': 12000, 'avg_loss': 5.6061855955785855, 'avg_acc': 50.061106018942866, 'loss': 6.1966071128845215}\n",
      "{'epoch': 2, 'iter': 14000, 'avg_loss': 5.605583243417396, 'avg_acc': 50.11487274718473, 'loss': 5.536513805389404}\n",
      "{'epoch': 2, 'iter': 16000, 'avg_loss': 5.605637013841425, 'avg_acc': 50.1166593754557, 'loss': 5.718729019165039}\n",
      "{'epoch': 2, 'iter': 18000, 'avg_loss': 5.606051224892977, 'avg_acc': 50.0578671555284, 'loss': 5.800928592681885}\n",
      "{'epoch': 2, 'iter': 20000, 'avg_loss': 5.6041191775264405, 'avg_acc': 50.05999700014999, 'loss': 5.107655048370361}\n",
      "{'epoch': 2, 'iter': 22000, 'avg_loss': 5.6052977648991185, 'avg_acc': 50.03711952487008, 'loss': 5.711782455444336}\n",
      "{'epoch': 2, 'iter': 24000, 'avg_loss': 5.604007901412676, 'avg_acc': 50.06770551227032, 'loss': 5.917311668395996}\n",
      "{'epoch': 2, 'iter': 26000, 'avg_loss': 5.602916870450594, 'avg_acc': 50.0099355153007, 'loss': 5.527393341064453}\n",
      "{'epoch': 2, 'iter': 28000, 'avg_loss': 5.60157019732437, 'avg_acc': 50.04523647964953, 'loss': 5.264788627624512}\n",
      "{'epoch': 2, 'iter': 30000, 'avg_loss': 5.601545098487435, 'avg_acc': 50.056664777840744, 'loss': 5.939075946807861}\n",
      "{'epoch': 2, 'iter': 32000, 'avg_loss': 5.601906386931492, 'avg_acc': 50.08879930835495, 'loss': 5.483392715454102}\n",
      "{'epoch': 2, 'iter': 34000, 'avg_loss': 5.602235037945098, 'avg_acc': 50.06470397929472, 'loss': 5.4008660316467285}\n",
      "{'epoch': 2, 'iter': 36000, 'avg_loss': 5.601689647765316, 'avg_acc': 50.06388711424683, 'loss': 5.7566728591918945}\n",
      "{'epoch': 2, 'iter': 38000, 'avg_loss': 5.6009241323628425, 'avg_acc': 50.06184047788216, 'loss': 6.123042106628418}\n",
      "{'epoch': 2, 'iter': 40000, 'avg_loss': 5.60029168497434, 'avg_acc': 50.036457421897786, 'loss': 5.579583168029785}\n",
      "{'epoch': 2, 'iter': 42000, 'avg_loss': 5.600488435358965, 'avg_acc': 50.05555423283573, 'loss': 5.8314690589904785}\n",
      "{'epoch': 2, 'iter': 44000, 'avg_loss': 5.5990979667576966, 'avg_acc': 50.03882487519223, 'loss': 5.398930072784424}\n",
      "{'epoch': 2, 'iter': 46000, 'avg_loss': 5.599327925555004, 'avg_acc': 50.06992601610109, 'loss': 5.593329429626465}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-8ceb8203f0db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f33d2cbcbb2f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f33d2cbcbb2f>\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_update_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_start = 1\n",
    "print(\"Training Start\")\n",
    "for epoch in range(epoch_start, epochs + epoch_start):\n",
    "    trainer.train(epoch)\n",
    "    trainer.save(epoch, output_path)\n",
    "\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:2 Model Saved on: ./models/bert_01.{0}.ep{1:2d}.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./models/bert_01.{0}.ep{1:2d}.pt'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n"
     ]
    }
   ],
   "source": [
    "print(\"Building BERT model\")\n",
    "bert = BERT(len(vocab), hidden=hidden, n_layers=layers, attn_heads=attn_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.load_state_dict(torch.load(\"./models/bert_01.bert.statedict.ep02.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_dataset[3]\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.model(inputs['bert_input'].unsqueeze(0), inputs['segment_label'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [UNK] [MASK] with positive sales in [UNK] , and was praised by both [UNK] and western critics . [UNK] release , it received downloadable content , [MASK] with an expanded edition in [UNK] of that year . [UNK] was [MASK] adapted ##gan manga and an original video animation series . [UNK] [MASK] low sales of [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] was not localized , but a fan translation compatible with the game [UNK] expanded edition was released in [MASK] . [UNK] would return to the franchise with the [MASK] of [UNK] : [UNK] [UNK] for the [UNK] 4 . \n",
      "\n",
      "\n",
      "\n",
      " [UNK] [UNK] travelled to the [UNK] [UNK] in [UNK] 2009 , to commence work on their seventh studio album , [UNK] 7 . [MASK] [MASK] a contract [MASK] [UNK] [UNK] [UNK] [UNK] label , [UNK] [UNK] , [MASK] in collaborations with [MASK] [UNK] profile producers . [UNK] late [UNK] 2009 , the [MASK] [MASK] [MASK] [MASK] were working with [UNK] [UNK] , known by his stage name [MASK] , on two songs . [UNK] [UNK] a [UNK] [UNK] was lice and produced by [UNK] , who ##teen the 2nd in collaboration with [UNK] [UNK] . [UNK] song [MASK] recorded lae [UNK] [UNK] in [UNK] [UNK] , [UNK] . [UNK] was mixed by [UNK] [UNK] [UNK] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in \" \".join([vocab.itos[t] for t in inputs['bert_input'].cpu().tolist() if t!=0]).split(\"[SEP]\"):\n",
    "    print(sent)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"is_next\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.5915,   0.0000]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
